{"cells":[{"cell_type":"markdown","metadata":{"id":"096fBrSoj3z8"},"source":["SUST Bangla"]},{"cell_type":"markdown","metadata":{"id":"Z05HYcen8siN"},"source":["# 7 Emotion Spiker Independent"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"pge_rpkxTEBX"},"outputs":[{"name":"stdout","output_type":"stream","text":["wav path:  data\\speech\n","emotion files:  ['data\\\\speech\\\\ANGRY_4-2-0-001.wav', 'data\\\\speech\\\\ANGRY_4-2-0-002.wav', 'data\\\\speech\\\\ANGRY_4-2-0-003.wav', 'data\\\\speech\\\\ANGRY_4-2-0-004.wav', 'data\\\\speech\\\\ANGRY_4-2-0-005.wav', 'data\\\\speech\\\\ANGRY_4-2-0-006.wav', 'data\\\\speech\\\\ANGRY_4-2-0-007.wav', 'data\\\\speech\\\\ANGRY_4-2-0-008.wav', 'data\\\\speech\\\\ANGRY_4-2-0-009.wav', 'data\\\\speech\\\\ANGRY_4-2-0-010.wav', 'data\\\\speech\\\\ANGRY_4-2-0-011.wav', 'data\\\\speech\\\\ANGRY_4-2-0-012.wav', 'data\\\\speech\\\\ANGRY_4-2-0-013.wav', 'data\\\\speech\\\\ANGRY_4-2-0-014.wav', 'data\\\\speech\\\\ANGRY_4-2-0-015.wav', 'data\\\\speech\\\\ANGRY_4-2-0-016.wav', 'data\\\\speech\\\\ANGRY_4-2-0-017.wav', 'data\\\\speech\\\\ANGRY_4-2-0-018.wav', 'data\\\\speech\\\\ANGRY_4-2-0-019.wav', 'data\\\\speech\\\\ANGRY_4-2-0-020.wav', 'data\\\\speech\\\\ANGRY_4-2-0-021.wav', 'data\\\\speech\\\\ANGRY_4-2-0-022.wav', 'data\\\\speech\\\\ANGRY_4-2-0-023.wav', 'data\\\\speech\\\\ANGRY_4-2-0-024.wav', 'data\\\\speech\\\\ANGRY_4-2-0-025.wav', 'data\\\\speech\\\\ANGRY_4-2-0-026.wav', 'data\\\\speech\\\\ANGRY_4-2-0-027.wav', 'data\\\\speech\\\\ANGRY_4-2-0-028.wav', 'data\\\\speech\\\\ANGRY_4-2-0-029.wav', 'data\\\\speech\\\\ANGRY_4-2-0-030.wav', 'data\\\\speech\\\\ANGRY_4-2-0-031.wav', 'data\\\\speech\\\\ANGRY_4-2-0-032.wav', 'data\\\\speech\\\\ANGRY_4-2-0-033.wav', 'data\\\\speech\\\\ANGRY_4-2-0-034.wav', 'data\\\\speech\\\\ANGRY_4-2-0-035.wav', 'data\\\\speech\\\\ANGRY_4-2-0-036.wav', 'data\\\\speech\\\\ANGRY_4-2-0-037.wav', 'data\\\\speech\\\\ANGRY_4-2-0-038.wav', 'data\\\\speech\\\\ANGRY_4-2-0-039.wav', 'data\\\\speech\\\\ANGRY_4-2-0-040.wav', 'data\\\\speech\\\\ANGRY_4-2-0-041.wav', 'data\\\\speech\\\\ANGRY_4-2-0-042.wav', 'data\\\\speech\\\\ANGRY_4-2-0-043.wav', 'data\\\\speech\\\\ANGRY_4-2-0-044.wav', 'data\\\\speech\\\\ANGRY_4-2-0-045.wav', 'data\\\\speech\\\\ANGRY_4-2-0-046.wav', 'data\\\\speech\\\\ANGRY_4-2-0-047.wav', 'data\\\\speech\\\\ANGRY_4-2-0-048.wav', 'data\\\\speech\\\\ANGRY_4-2-0-049.wav', 'data\\\\speech\\\\ANGRY_4-2-0-050.wav', 'data\\\\speech\\\\ANGRY_4-2-1-001.wav', 'data\\\\speech\\\\ANGRY_4-2-1-002.wav', 'data\\\\speech\\\\ANGRY_4-2-1-003.wav', 'data\\\\speech\\\\ANGRY_4-2-1-004.wav', 'data\\\\speech\\\\ANGRY_4-2-1-005.wav', 'data\\\\speech\\\\ANGRY_4-2-1-006.wav', 'data\\\\speech\\\\ANGRY_4-2-1-007.wav', 'data\\\\speech\\\\ANGRY_4-2-1-008.wav', 'data\\\\speech\\\\ANGRY_4-2-1-009.wav', 'data\\\\speech\\\\ANGRY_4-2-1-010.wav', 'data\\\\speech\\\\ANGRY_4-2-1-011.wav', 'data\\\\speech\\\\ANGRY_4-2-1-012.wav', 'data\\\\speech\\\\ANGRY_4-2-1-013.wav', 'data\\\\speech\\\\ANGRY_4-2-1-014.wav', 'data\\\\speech\\\\ANGRY_4-2-1-015.wav', 'data\\\\speech\\\\ANGRY_4-2-1-016.wav', 'data\\\\speech\\\\ANGRY_4-2-1-017.wav', 'data\\\\speech\\\\ANGRY_4-2-1-018.wav', 'data\\\\speech\\\\ANGRY_4-2-1-019.wav', 'data\\\\speech\\\\ANGRY_4-2-1-020.wav', 'data\\\\speech\\\\ANGRY_4-2-1-021.wav', 'data\\\\speech\\\\ANGRY_4-2-1-022.wav', 'data\\\\speech\\\\ANGRY_4-2-1-023.wav', 'data\\\\speech\\\\ANGRY_4-2-1-024.wav', 'data\\\\speech\\\\ANGRY_4-2-1-025.wav', 'data\\\\speech\\\\ANGRY_4-2-1-026.wav', 'data\\\\speech\\\\ANGRY_4-2-1-027.wav', 'data\\\\speech\\\\ANGRY_4-2-1-028.wav', 'data\\\\speech\\\\ANGRY_4-2-1-029.wav', 'data\\\\speech\\\\ANGRY_4-2-1-030.wav', 'data\\\\speech\\\\ANGRY_4-2-1-031.wav', 'data\\\\speech\\\\ANGRY_4-2-1-032.wav', 'data\\\\speech\\\\ANGRY_4-2-1-033.wav', 'data\\\\speech\\\\ANGRY_4-2-1-034.wav', 'data\\\\speech\\\\ANGRY_4-2-1-035.wav', 'data\\\\speech\\\\ANGRY_4-2-1-036.wav', 'data\\\\speech\\\\ANGRY_4-2-1-037.wav', 'data\\\\speech\\\\ANGRY_4-2-1-038.wav', 'data\\\\speech\\\\ANGRY_4-2-1-039.wav', 'data\\\\speech\\\\ANGRY_4-2-1-040.wav', 'data\\\\speech\\\\ANGRY_4-2-1-041.wav', 'data\\\\speech\\\\ANGRY_4-2-1-042.wav', 'data\\\\speech\\\\ANGRY_4-2-1-043.wav', 'data\\\\speech\\\\ANGRY_4-2-1-044.wav', 'data\\\\speech\\\\ANGRY_4-2-1-045.wav', 'data\\\\speech\\\\ANGRY_4-2-1-046.wav', 'data\\\\speech\\\\ANGRY_4-2-1-047.wav', 'data\\\\speech\\\\ANGRY_4-2-1-048.wav', 'data\\\\speech\\\\ANGRY_4-2-1-049.wav', 'data\\\\speech\\\\ANGRY_4-2-1-050.wav', 'data\\\\speech\\\\DISGUST_5-2-0-001.wav', 'data\\\\speech\\\\DISGUST_5-2-0-002.wav', 'data\\\\speech\\\\DISGUST_5-2-0-003.wav', 'data\\\\speech\\\\DISGUST_5-2-0-004.wav', 'data\\\\speech\\\\DISGUST_5-2-0-005.wav', 'data\\\\speech\\\\DISGUST_5-2-0-006.wav', 'data\\\\speech\\\\DISGUST_5-2-0-007.wav', 'data\\\\speech\\\\DISGUST_5-2-0-008.wav', 'data\\\\speech\\\\DISGUST_5-2-0-009.wav', 'data\\\\speech\\\\DISGUST_5-2-0-010.wav', 'data\\\\speech\\\\DISGUST_5-2-0-011.wav', 'data\\\\speech\\\\DISGUST_5-2-0-012.wav', 'data\\\\speech\\\\DISGUST_5-2-0-013.wav', 'data\\\\speech\\\\DISGUST_5-2-0-014.wav', 'data\\\\speech\\\\DISGUST_5-2-0-015.wav', 'data\\\\speech\\\\DISGUST_5-2-0-016.wav', 'data\\\\speech\\\\DISGUST_5-2-0-017.wav', 'data\\\\speech\\\\DISGUST_5-2-0-018.wav', 'data\\\\speech\\\\DISGUST_5-2-0-019.wav', 'data\\\\speech\\\\DISGUST_5-2-0-020.wav', 'data\\\\speech\\\\DISGUST_5-2-0-021.wav', 'data\\\\speech\\\\DISGUST_5-2-0-022.wav', 'data\\\\speech\\\\DISGUST_5-2-0-023.wav', 'data\\\\speech\\\\DISGUST_5-2-0-024.wav', 'data\\\\speech\\\\DISGUST_5-2-0-025.wav', 'data\\\\speech\\\\DISGUST_5-2-0-026.wav', 'data\\\\speech\\\\DISGUST_5-2-0-027.wav', 'data\\\\speech\\\\DISGUST_5-2-0-028.wav', 'data\\\\speech\\\\DISGUST_5-2-0-029.wav', 'data\\\\speech\\\\DISGUST_5-2-0-030.wav', 'data\\\\speech\\\\DISGUST_5-2-0-031.wav', 'data\\\\speech\\\\DISGUST_5-2-0-032.wav', 'data\\\\speech\\\\DISGUST_5-2-0-033.wav', 'data\\\\speech\\\\DISGUST_5-2-0-034.wav', 'data\\\\speech\\\\DISGUST_5-2-0-035.wav', 'data\\\\speech\\\\DISGUST_5-2-0-036.wav', 'data\\\\speech\\\\DISGUST_5-2-0-037.wav', 'data\\\\speech\\\\DISGUST_5-2-0-038.wav', 'data\\\\speech\\\\DISGUST_5-2-0-039.wav', 'data\\\\speech\\\\DISGUST_5-2-0-040.wav', 'data\\\\speech\\\\DISGUST_5-2-0-041.wav', 'data\\\\speech\\\\DISGUST_5-2-0-042.wav', 'data\\\\speech\\\\DISGUST_5-2-0-043.wav', 'data\\\\speech\\\\DISGUST_5-2-0-044.wav', 'data\\\\speech\\\\DISGUST_5-2-0-045.wav', 'data\\\\speech\\\\DISGUST_5-2-0-046.wav', 'data\\\\speech\\\\DISGUST_5-2-0-047.wav', 'data\\\\speech\\\\DISGUST_5-2-0-048.wav', 'data\\\\speech\\\\DISGUST_5-2-0-049.wav', 'data\\\\speech\\\\DISGUST_5-2-0-050.wav', 'data\\\\speech\\\\DISGUST_5-2-1-001.wav', 'data\\\\speech\\\\DISGUST_5-2-1-002.wav', 'data\\\\speech\\\\DISGUST_5-2-1-003.wav', 'data\\\\speech\\\\DISGUST_5-2-1-004.wav', 'data\\\\speech\\\\DISGUST_5-2-1-005.wav', 'data\\\\speech\\\\DISGUST_5-2-1-006.wav', 'data\\\\speech\\\\DISGUST_5-2-1-007.wav', 'data\\\\speech\\\\DISGUST_5-2-1-008.wav', 'data\\\\speech\\\\DISGUST_5-2-1-009.wav', 'data\\\\speech\\\\DISGUST_5-2-1-010.wav', 'data\\\\speech\\\\DISGUST_5-2-1-011.wav', 'data\\\\speech\\\\DISGUST_5-2-1-012.wav', 'data\\\\speech\\\\DISGUST_5-2-1-013.wav', 'data\\\\speech\\\\DISGUST_5-2-1-014.wav', 'data\\\\speech\\\\DISGUST_5-2-1-015.wav', 'data\\\\speech\\\\DISGUST_5-2-1-016.wav', 'data\\\\speech\\\\DISGUST_5-2-1-017.wav', 'data\\\\speech\\\\DISGUST_5-2-1-018.wav', 'data\\\\speech\\\\DISGUST_5-2-1-019.wav', 'data\\\\speech\\\\DISGUST_5-2-1-020.wav', 'data\\\\speech\\\\DISGUST_5-2-1-021.wav', 'data\\\\speech\\\\DISGUST_5-2-1-022.wav', 'data\\\\speech\\\\DISGUST_5-2-1-023.wav', 'data\\\\speech\\\\DISGUST_5-2-1-024.wav', 'data\\\\speech\\\\DISGUST_5-2-1-025.wav', 'data\\\\speech\\\\DISGUST_5-2-1-026.wav', 'data\\\\speech\\\\DISGUST_5-2-1-027.wav', 'data\\\\speech\\\\DISGUST_5-2-1-028.wav', 'data\\\\speech\\\\DISGUST_5-2-1-029.wav', 'data\\\\speech\\\\DISGUST_5-2-1-030.wav', 'data\\\\speech\\\\DISGUST_5-2-1-031.wav', 'data\\\\speech\\\\DISGUST_5-2-1-032.wav', 'data\\\\speech\\\\DISGUST_5-2-1-033.wav', 'data\\\\speech\\\\DISGUST_5-2-1-034.wav', 'data\\\\speech\\\\DISGUST_5-2-1-035.wav', 'data\\\\speech\\\\DISGUST_5-2-1-036.wav', 'data\\\\speech\\\\DISGUST_5-2-1-037.wav', 'data\\\\speech\\\\DISGUST_5-2-1-038.wav', 'data\\\\speech\\\\DISGUST_5-2-1-039.wav', 'data\\\\speech\\\\DISGUST_5-2-1-040.wav', 'data\\\\speech\\\\DISGUST_5-2-1-041.wav', 'data\\\\speech\\\\DISGUST_5-2-1-042.wav', 'data\\\\speech\\\\DISGUST_5-2-1-043.wav', 'data\\\\speech\\\\DISGUST_5-2-1-044.wav', 'data\\\\speech\\\\DISGUST_5-2-1-045.wav', 'data\\\\speech\\\\DISGUST_5-2-1-046.wav', 'data\\\\speech\\\\DISGUST_5-2-1-047.wav', 'data\\\\speech\\\\DISGUST_5-2-1-048.wav', 'data\\\\speech\\\\DISGUST_5-2-1-049.wav', 'data\\\\speech\\\\DISGUST_5-2-1-050.wav', 'data\\\\speech\\\\HAPPY_2-2-0-001.wav', 'data\\\\speech\\\\HAPPY_2-2-0-002.wav', 'data\\\\speech\\\\HAPPY_2-2-0-003.wav', 'data\\\\speech\\\\HAPPY_2-2-0-004.wav', 'data\\\\speech\\\\HAPPY_2-2-0-005.wav', 'data\\\\speech\\\\HAPPY_2-2-0-006.wav', 'data\\\\speech\\\\HAPPY_2-2-0-007.wav', 'data\\\\speech\\\\HAPPY_2-2-0-008.wav', 'data\\\\speech\\\\HAPPY_2-2-0-009.wav', 'data\\\\speech\\\\HAPPY_2-2-0-010.wav', 'data\\\\speech\\\\HAPPY_2-2-0-011.wav', 'data\\\\speech\\\\HAPPY_2-2-0-012.wav', 'data\\\\speech\\\\HAPPY_2-2-0-013.wav', 'data\\\\speech\\\\HAPPY_2-2-0-014.wav', 'data\\\\speech\\\\HAPPY_2-2-0-015.wav', 'data\\\\speech\\\\HAPPY_2-2-0-016.wav', 'data\\\\speech\\\\HAPPY_2-2-0-017.wav', 'data\\\\speech\\\\HAPPY_2-2-0-018.wav', 'data\\\\speech\\\\HAPPY_2-2-0-019.wav', 'data\\\\speech\\\\HAPPY_2-2-0-020.wav', 'data\\\\speech\\\\HAPPY_2-2-0-021.wav', 'data\\\\speech\\\\HAPPY_2-2-0-022.wav', 'data\\\\speech\\\\HAPPY_2-2-0-023.wav', 'data\\\\speech\\\\HAPPY_2-2-0-024.wav', 'data\\\\speech\\\\HAPPY_2-2-0-025.wav', 'data\\\\speech\\\\HAPPY_2-2-0-026.wav', 'data\\\\speech\\\\HAPPY_2-2-0-027.wav', 'data\\\\speech\\\\HAPPY_2-2-0-028.wav', 'data\\\\speech\\\\HAPPY_2-2-0-029.wav', 'data\\\\speech\\\\HAPPY_2-2-0-030.wav', 'data\\\\speech\\\\HAPPY_2-2-0-031.wav', 'data\\\\speech\\\\HAPPY_2-2-0-032.wav', 'data\\\\speech\\\\HAPPY_2-2-0-033.wav', 'data\\\\speech\\\\HAPPY_2-2-0-034.wav', 'data\\\\speech\\\\HAPPY_2-2-0-035.wav', 'data\\\\speech\\\\HAPPY_2-2-0-036.wav', 'data\\\\speech\\\\HAPPY_2-2-0-037.wav', 'data\\\\speech\\\\HAPPY_2-2-0-038.wav', 'data\\\\speech\\\\HAPPY_2-2-0-039.wav', 'data\\\\speech\\\\HAPPY_2-2-0-040.wav', 'data\\\\speech\\\\HAPPY_2-2-0-041.wav', 'data\\\\speech\\\\HAPPY_2-2-0-042.wav', 'data\\\\speech\\\\HAPPY_2-2-0-043.wav', 'data\\\\speech\\\\HAPPY_2-2-0-044.wav', 'data\\\\speech\\\\HAPPY_2-2-0-045.wav', 'data\\\\speech\\\\HAPPY_2-2-0-046.wav', 'data\\\\speech\\\\HAPPY_2-2-0-047.wav', 'data\\\\speech\\\\HAPPY_2-2-0-048.wav', 'data\\\\speech\\\\HAPPY_2-2-0-049.wav', 'data\\\\speech\\\\HAPPY_2-2-0-050.wav', 'data\\\\speech\\\\HAPPY_2-2-1-001.wav', 'data\\\\speech\\\\HAPPY_2-2-1-002.wav', 'data\\\\speech\\\\HAPPY_2-2-1-003.wav', 'data\\\\speech\\\\HAPPY_2-2-1-004.wav', 'data\\\\speech\\\\HAPPY_2-2-1-005.wav', 'data\\\\speech\\\\HAPPY_2-2-1-006.wav', 'data\\\\speech\\\\HAPPY_2-2-1-007.wav', 'data\\\\speech\\\\HAPPY_2-2-1-008.wav', 'data\\\\speech\\\\HAPPY_2-2-1-009.wav', 'data\\\\speech\\\\HAPPY_2-2-1-010.wav', 'data\\\\speech\\\\HAPPY_2-2-1-011.wav', 'data\\\\speech\\\\HAPPY_2-2-1-012.wav', 'data\\\\speech\\\\HAPPY_2-2-1-013.wav', 'data\\\\speech\\\\HAPPY_2-2-1-014.wav', 'data\\\\speech\\\\HAPPY_2-2-1-015.wav', 'data\\\\speech\\\\HAPPY_2-2-1-016.wav', 'data\\\\speech\\\\HAPPY_2-2-1-017.wav', 'data\\\\speech\\\\HAPPY_2-2-1-018.wav', 'data\\\\speech\\\\HAPPY_2-2-1-019.wav', 'data\\\\speech\\\\HAPPY_2-2-1-020.wav', 'data\\\\speech\\\\HAPPY_2-2-1-021.wav', 'data\\\\speech\\\\HAPPY_2-2-1-022.wav', 'data\\\\speech\\\\HAPPY_2-2-1-023.wav', 'data\\\\speech\\\\HAPPY_2-2-1-024.wav', 'data\\\\speech\\\\HAPPY_2-2-1-025.wav', 'data\\\\speech\\\\HAPPY_2-2-1-026.wav', 'data\\\\speech\\\\HAPPY_2-2-1-027.wav', 'data\\\\speech\\\\HAPPY_2-2-1-028.wav', 'data\\\\speech\\\\HAPPY_2-2-1-029.wav', 'data\\\\speech\\\\HAPPY_2-2-1-030.wav', 'data\\\\speech\\\\HAPPY_2-2-1-031.wav', 'data\\\\speech\\\\HAPPY_2-2-1-032.wav', 'data\\\\speech\\\\HAPPY_2-2-1-033.wav', 'data\\\\speech\\\\HAPPY_2-2-1-034.wav', 'data\\\\speech\\\\HAPPY_2-2-1-035.wav', 'data\\\\speech\\\\HAPPY_2-2-1-036.wav', 'data\\\\speech\\\\HAPPY_2-2-1-037.wav', 'data\\\\speech\\\\HAPPY_2-2-1-038.wav', 'data\\\\speech\\\\HAPPY_2-2-1-039.wav', 'data\\\\speech\\\\HAPPY_2-2-1-040.wav', 'data\\\\speech\\\\HAPPY_2-2-1-041.wav', 'data\\\\speech\\\\HAPPY_2-2-1-042.wav', 'data\\\\speech\\\\HAPPY_2-2-1-043.wav', 'data\\\\speech\\\\HAPPY_2-2-1-044.wav', 'data\\\\speech\\\\HAPPY_2-2-1-045.wav', 'data\\\\speech\\\\HAPPY_2-2-1-046.wav', 'data\\\\speech\\\\HAPPY_2-2-1-047.wav', 'data\\\\speech\\\\HAPPY_2-2-1-048.wav', 'data\\\\speech\\\\HAPPY_2-2-1-049.wav', 'data\\\\speech\\\\HAPPY_2-2-1-050.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-001.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-002.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-003.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-004.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-005.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-006.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-007.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-008.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-009.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-010.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-011.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-012.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-013.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-014.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-015.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-016.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-017.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-018.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-019.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-020.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-021.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-022.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-023.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-024.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-025.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-026.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-027.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-028.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-029.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-030.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-031.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-032.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-033.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-034.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-035.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-036.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-037.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-038.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-039.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-040.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-041.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-042.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-043.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-044.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-045.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-046.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-047.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-048.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-049.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-050.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-001.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-002.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-003.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-004.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-005.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-006.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-007.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-008.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-009.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-010.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-011.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-012.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-013.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-014.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-015.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-016.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-017.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-018.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-019.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-020.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-021.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-022.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-023.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-024.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-025.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-026.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-027.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-028.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-029.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-030.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-031.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-032.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-033.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-034.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-035.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-036.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-037.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-038.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-039.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-040.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-041.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-042.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-043.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-044.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-045.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-046.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-047.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-048.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-049.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-050.wav', 'data\\\\speech\\\\SAD_3-2-0-001.wav', 'data\\\\speech\\\\SAD_3-2-0-002.wav', 'data\\\\speech\\\\SAD_3-2-0-003.wav', 'data\\\\speech\\\\SAD_3-2-0-004.wav', 'data\\\\speech\\\\SAD_3-2-0-005.wav', 'data\\\\speech\\\\SAD_3-2-0-006.wav', 'data\\\\speech\\\\SAD_3-2-0-007.wav', 'data\\\\speech\\\\SAD_3-2-0-008.wav', 'data\\\\speech\\\\SAD_3-2-0-009.wav', 'data\\\\speech\\\\SAD_3-2-0-010.wav', 'data\\\\speech\\\\SAD_3-2-0-011.wav', 'data\\\\speech\\\\SAD_3-2-0-012.wav', 'data\\\\speech\\\\SAD_3-2-0-013.wav', 'data\\\\speech\\\\SAD_3-2-0-014.wav', 'data\\\\speech\\\\SAD_3-2-0-015.wav', 'data\\\\speech\\\\SAD_3-2-0-016.wav', 'data\\\\speech\\\\SAD_3-2-0-017.wav', 'data\\\\speech\\\\SAD_3-2-0-018.wav', 'data\\\\speech\\\\SAD_3-2-0-019.wav', 'data\\\\speech\\\\SAD_3-2-0-020.wav', 'data\\\\speech\\\\SAD_3-2-0-021.wav', 'data\\\\speech\\\\SAD_3-2-0-022.wav', 'data\\\\speech\\\\SAD_3-2-0-023.wav', 'data\\\\speech\\\\SAD_3-2-0-024.wav', 'data\\\\speech\\\\SAD_3-2-0-025.wav', 'data\\\\speech\\\\SAD_3-2-0-026.wav', 'data\\\\speech\\\\SAD_3-2-0-027.wav', 'data\\\\speech\\\\SAD_3-2-0-028.wav', 'data\\\\speech\\\\SAD_3-2-0-029.wav', 'data\\\\speech\\\\SAD_3-2-0-030.wav', 'data\\\\speech\\\\SAD_3-2-0-031.wav', 'data\\\\speech\\\\SAD_3-2-0-032.wav', 'data\\\\speech\\\\SAD_3-2-0-033.wav', 'data\\\\speech\\\\SAD_3-2-0-034.wav', 'data\\\\speech\\\\SAD_3-2-0-035.wav', 'data\\\\speech\\\\SAD_3-2-0-036.wav', 'data\\\\speech\\\\SAD_3-2-0-037.wav', 'data\\\\speech\\\\SAD_3-2-0-038.wav', 'data\\\\speech\\\\SAD_3-2-0-039.wav', 'data\\\\speech\\\\SAD_3-2-0-040.wav', 'data\\\\speech\\\\SAD_3-2-0-041.wav', 'data\\\\speech\\\\SAD_3-2-0-042.wav', 'data\\\\speech\\\\SAD_3-2-0-043.wav', 'data\\\\speech\\\\SAD_3-2-0-044.wav', 'data\\\\speech\\\\SAD_3-2-0-045.wav', 'data\\\\speech\\\\SAD_3-2-0-046.wav', 'data\\\\speech\\\\SAD_3-2-0-047.wav', 'data\\\\speech\\\\SAD_3-2-0-048.wav', 'data\\\\speech\\\\SAD_3-2-0-049.wav', 'data\\\\speech\\\\SAD_3-2-0-050.wav', 'data\\\\speech\\\\SAD_3-2-1-001.wav', 'data\\\\speech\\\\SAD_3-2-1-002.wav', 'data\\\\speech\\\\SAD_3-2-1-003.wav', 'data\\\\speech\\\\SAD_3-2-1-004.wav', 'data\\\\speech\\\\SAD_3-2-1-005.wav', 'data\\\\speech\\\\SAD_3-2-1-006.wav', 'data\\\\speech\\\\SAD_3-2-1-007.wav', 'data\\\\speech\\\\SAD_3-2-1-008.wav', 'data\\\\speech\\\\SAD_3-2-1-009.wav', 'data\\\\speech\\\\SAD_3-2-1-010.wav', 'data\\\\speech\\\\SAD_3-2-1-011.wav', 'data\\\\speech\\\\SAD_3-2-1-012.wav', 'data\\\\speech\\\\SAD_3-2-1-013.wav', 'data\\\\speech\\\\SAD_3-2-1-014.wav', 'data\\\\speech\\\\SAD_3-2-1-015.wav', 'data\\\\speech\\\\SAD_3-2-1-016.wav', 'data\\\\speech\\\\SAD_3-2-1-017.wav', 'data\\\\speech\\\\SAD_3-2-1-018.wav', 'data\\\\speech\\\\SAD_3-2-1-019.wav', 'data\\\\speech\\\\SAD_3-2-1-020.wav', 'data\\\\speech\\\\SAD_3-2-1-021.wav', 'data\\\\speech\\\\SAD_3-2-1-022.wav', 'data\\\\speech\\\\SAD_3-2-1-023.wav', 'data\\\\speech\\\\SAD_3-2-1-024.wav', 'data\\\\speech\\\\SAD_3-2-1-025.wav', 'data\\\\speech\\\\SAD_3-2-1-026.wav', 'data\\\\speech\\\\SAD_3-2-1-027.wav', 'data\\\\speech\\\\SAD_3-2-1-028.wav', 'data\\\\speech\\\\SAD_3-2-1-029.wav', 'data\\\\speech\\\\SAD_3-2-1-030.wav', 'data\\\\speech\\\\SAD_3-2-1-031.wav', 'data\\\\speech\\\\SAD_3-2-1-032.wav', 'data\\\\speech\\\\SAD_3-2-1-033.wav', 'data\\\\speech\\\\SAD_3-2-1-034.wav', 'data\\\\speech\\\\SAD_3-2-1-035.wav', 'data\\\\speech\\\\SAD_3-2-1-036.wav', 'data\\\\speech\\\\SAD_3-2-1-037.wav', 'data\\\\speech\\\\SAD_3-2-1-038.wav', 'data\\\\speech\\\\SAD_3-2-1-039.wav', 'data\\\\speech\\\\SAD_3-2-1-040.wav', 'data\\\\speech\\\\SAD_3-2-1-041.wav', 'data\\\\speech\\\\SAD_3-2-1-042.wav', 'data\\\\speech\\\\SAD_3-2-1-043.wav', 'data\\\\speech\\\\SAD_3-2-1-044.wav', 'data\\\\speech\\\\SAD_3-2-1-045.wav', 'data\\\\speech\\\\SAD_3-2-1-046.wav', 'data\\\\speech\\\\SAD_3-2-1-047.wav', 'data\\\\speech\\\\SAD_3-2-1-048.wav', 'data\\\\speech\\\\SAD_3-2-1-049.wav', 'data\\\\speech\\\\SAD_3-2-1-050.wav']\n","dict_keys(['ANGRY', 'DISGUST', 'HAPPY', 'NEUTRAL', 'SAD'])\n","ANGRY\n","['data\\\\speech\\\\ANGRY_4-2-0-001.wav', 'data\\\\speech\\\\ANGRY_4-2-0-002.wav', 'data\\\\speech\\\\ANGRY_4-2-0-003.wav', 'data\\\\speech\\\\ANGRY_4-2-0-004.wav', 'data\\\\speech\\\\ANGRY_4-2-0-005.wav', 'data\\\\speech\\\\ANGRY_4-2-0-006.wav', 'data\\\\speech\\\\ANGRY_4-2-0-007.wav', 'data\\\\speech\\\\ANGRY_4-2-0-008.wav', 'data\\\\speech\\\\ANGRY_4-2-0-009.wav', 'data\\\\speech\\\\ANGRY_4-2-0-010.wav', 'data\\\\speech\\\\ANGRY_4-2-0-011.wav', 'data\\\\speech\\\\ANGRY_4-2-0-012.wav', 'data\\\\speech\\\\ANGRY_4-2-0-013.wav', 'data\\\\speech\\\\ANGRY_4-2-0-014.wav', 'data\\\\speech\\\\ANGRY_4-2-0-015.wav', 'data\\\\speech\\\\ANGRY_4-2-0-016.wav', 'data\\\\speech\\\\ANGRY_4-2-0-017.wav', 'data\\\\speech\\\\ANGRY_4-2-0-018.wav', 'data\\\\speech\\\\ANGRY_4-2-0-019.wav', 'data\\\\speech\\\\ANGRY_4-2-0-020.wav', 'data\\\\speech\\\\ANGRY_4-2-0-021.wav', 'data\\\\speech\\\\ANGRY_4-2-0-022.wav', 'data\\\\speech\\\\ANGRY_4-2-0-023.wav', 'data\\\\speech\\\\ANGRY_4-2-0-024.wav', 'data\\\\speech\\\\ANGRY_4-2-0-025.wav', 'data\\\\speech\\\\ANGRY_4-2-0-026.wav', 'data\\\\speech\\\\ANGRY_4-2-0-027.wav', 'data\\\\speech\\\\ANGRY_4-2-0-028.wav', 'data\\\\speech\\\\ANGRY_4-2-0-029.wav', 'data\\\\speech\\\\ANGRY_4-2-0-030.wav', 'data\\\\speech\\\\ANGRY_4-2-0-031.wav', 'data\\\\speech\\\\ANGRY_4-2-0-032.wav', 'data\\\\speech\\\\ANGRY_4-2-0-033.wav', 'data\\\\speech\\\\ANGRY_4-2-0-034.wav', 'data\\\\speech\\\\ANGRY_4-2-0-035.wav', 'data\\\\speech\\\\ANGRY_4-2-0-036.wav', 'data\\\\speech\\\\ANGRY_4-2-0-037.wav', 'data\\\\speech\\\\ANGRY_4-2-0-038.wav', 'data\\\\speech\\\\ANGRY_4-2-0-039.wav', 'data\\\\speech\\\\ANGRY_4-2-0-040.wav', 'data\\\\speech\\\\ANGRY_4-2-0-041.wav', 'data\\\\speech\\\\ANGRY_4-2-0-042.wav', 'data\\\\speech\\\\ANGRY_4-2-0-043.wav', 'data\\\\speech\\\\ANGRY_4-2-0-044.wav', 'data\\\\speech\\\\ANGRY_4-2-0-045.wav', 'data\\\\speech\\\\ANGRY_4-2-0-046.wav', 'data\\\\speech\\\\ANGRY_4-2-0-047.wav', 'data\\\\speech\\\\ANGRY_4-2-0-048.wav', 'data\\\\speech\\\\ANGRY_4-2-0-049.wav', 'data\\\\speech\\\\ANGRY_4-2-0-050.wav', 'data\\\\speech\\\\ANGRY_4-2-1-001.wav', 'data\\\\speech\\\\ANGRY_4-2-1-002.wav', 'data\\\\speech\\\\ANGRY_4-2-1-003.wav', 'data\\\\speech\\\\ANGRY_4-2-1-004.wav', 'data\\\\speech\\\\ANGRY_4-2-1-005.wav', 'data\\\\speech\\\\ANGRY_4-2-1-006.wav', 'data\\\\speech\\\\ANGRY_4-2-1-007.wav', 'data\\\\speech\\\\ANGRY_4-2-1-008.wav', 'data\\\\speech\\\\ANGRY_4-2-1-009.wav', 'data\\\\speech\\\\ANGRY_4-2-1-010.wav', 'data\\\\speech\\\\ANGRY_4-2-1-011.wav', 'data\\\\speech\\\\ANGRY_4-2-1-012.wav', 'data\\\\speech\\\\ANGRY_4-2-1-013.wav', 'data\\\\speech\\\\ANGRY_4-2-1-014.wav', 'data\\\\speech\\\\ANGRY_4-2-1-015.wav', 'data\\\\speech\\\\ANGRY_4-2-1-016.wav', 'data\\\\speech\\\\ANGRY_4-2-1-017.wav', 'data\\\\speech\\\\ANGRY_4-2-1-018.wav', 'data\\\\speech\\\\ANGRY_4-2-1-019.wav', 'data\\\\speech\\\\ANGRY_4-2-1-020.wav', 'data\\\\speech\\\\ANGRY_4-2-1-021.wav', 'data\\\\speech\\\\ANGRY_4-2-1-022.wav', 'data\\\\speech\\\\ANGRY_4-2-1-023.wav', 'data\\\\speech\\\\ANGRY_4-2-1-024.wav', 'data\\\\speech\\\\ANGRY_4-2-1-025.wav', 'data\\\\speech\\\\ANGRY_4-2-1-026.wav', 'data\\\\speech\\\\ANGRY_4-2-1-027.wav', 'data\\\\speech\\\\ANGRY_4-2-1-028.wav', 'data\\\\speech\\\\ANGRY_4-2-1-029.wav', 'data\\\\speech\\\\ANGRY_4-2-1-030.wav', 'data\\\\speech\\\\ANGRY_4-2-1-031.wav', 'data\\\\speech\\\\ANGRY_4-2-1-032.wav', 'data\\\\speech\\\\ANGRY_4-2-1-033.wav', 'data\\\\speech\\\\ANGRY_4-2-1-034.wav', 'data\\\\speech\\\\ANGRY_4-2-1-035.wav', 'data\\\\speech\\\\ANGRY_4-2-1-036.wav', 'data\\\\speech\\\\ANGRY_4-2-1-037.wav', 'data\\\\speech\\\\ANGRY_4-2-1-038.wav', 'data\\\\speech\\\\ANGRY_4-2-1-039.wav', 'data\\\\speech\\\\ANGRY_4-2-1-040.wav', 'data\\\\speech\\\\ANGRY_4-2-1-041.wav', 'data\\\\speech\\\\ANGRY_4-2-1-042.wav', 'data\\\\speech\\\\ANGRY_4-2-1-043.wav', 'data\\\\speech\\\\ANGRY_4-2-1-044.wav', 'data\\\\speech\\\\ANGRY_4-2-1-045.wav', 'data\\\\speech\\\\ANGRY_4-2-1-046.wav', 'data\\\\speech\\\\ANGRY_4-2-1-047.wav', 'data\\\\speech\\\\ANGRY_4-2-1-048.wav', 'data\\\\speech\\\\ANGRY_4-2-1-049.wav', 'data\\\\speech\\\\ANGRY_4-2-1-050.wav']\n","(100, 98304)\n","DISGUST\n","['data\\\\speech\\\\DISGUST_5-2-0-001.wav', 'data\\\\speech\\\\DISGUST_5-2-0-002.wav', 'data\\\\speech\\\\DISGUST_5-2-0-003.wav', 'data\\\\speech\\\\DISGUST_5-2-0-004.wav', 'data\\\\speech\\\\DISGUST_5-2-0-005.wav', 'data\\\\speech\\\\DISGUST_5-2-0-006.wav', 'data\\\\speech\\\\DISGUST_5-2-0-007.wav', 'data\\\\speech\\\\DISGUST_5-2-0-008.wav', 'data\\\\speech\\\\DISGUST_5-2-0-009.wav', 'data\\\\speech\\\\DISGUST_5-2-0-010.wav', 'data\\\\speech\\\\DISGUST_5-2-0-011.wav', 'data\\\\speech\\\\DISGUST_5-2-0-012.wav', 'data\\\\speech\\\\DISGUST_5-2-0-013.wav', 'data\\\\speech\\\\DISGUST_5-2-0-014.wav', 'data\\\\speech\\\\DISGUST_5-2-0-015.wav', 'data\\\\speech\\\\DISGUST_5-2-0-016.wav', 'data\\\\speech\\\\DISGUST_5-2-0-017.wav', 'data\\\\speech\\\\DISGUST_5-2-0-018.wav', 'data\\\\speech\\\\DISGUST_5-2-0-019.wav', 'data\\\\speech\\\\DISGUST_5-2-0-020.wav', 'data\\\\speech\\\\DISGUST_5-2-0-021.wav', 'data\\\\speech\\\\DISGUST_5-2-0-022.wav', 'data\\\\speech\\\\DISGUST_5-2-0-023.wav', 'data\\\\speech\\\\DISGUST_5-2-0-024.wav', 'data\\\\speech\\\\DISGUST_5-2-0-025.wav', 'data\\\\speech\\\\DISGUST_5-2-0-026.wav', 'data\\\\speech\\\\DISGUST_5-2-0-027.wav', 'data\\\\speech\\\\DISGUST_5-2-0-028.wav', 'data\\\\speech\\\\DISGUST_5-2-0-029.wav', 'data\\\\speech\\\\DISGUST_5-2-0-030.wav', 'data\\\\speech\\\\DISGUST_5-2-0-031.wav', 'data\\\\speech\\\\DISGUST_5-2-0-032.wav', 'data\\\\speech\\\\DISGUST_5-2-0-033.wav', 'data\\\\speech\\\\DISGUST_5-2-0-034.wav', 'data\\\\speech\\\\DISGUST_5-2-0-035.wav', 'data\\\\speech\\\\DISGUST_5-2-0-036.wav', 'data\\\\speech\\\\DISGUST_5-2-0-037.wav', 'data\\\\speech\\\\DISGUST_5-2-0-038.wav', 'data\\\\speech\\\\DISGUST_5-2-0-039.wav', 'data\\\\speech\\\\DISGUST_5-2-0-040.wav', 'data\\\\speech\\\\DISGUST_5-2-0-041.wav', 'data\\\\speech\\\\DISGUST_5-2-0-042.wav', 'data\\\\speech\\\\DISGUST_5-2-0-043.wav', 'data\\\\speech\\\\DISGUST_5-2-0-044.wav', 'data\\\\speech\\\\DISGUST_5-2-0-045.wav', 'data\\\\speech\\\\DISGUST_5-2-0-046.wav', 'data\\\\speech\\\\DISGUST_5-2-0-047.wav', 'data\\\\speech\\\\DISGUST_5-2-0-048.wav', 'data\\\\speech\\\\DISGUST_5-2-0-049.wav', 'data\\\\speech\\\\DISGUST_5-2-0-050.wav', 'data\\\\speech\\\\DISGUST_5-2-1-001.wav', 'data\\\\speech\\\\DISGUST_5-2-1-002.wav', 'data\\\\speech\\\\DISGUST_5-2-1-003.wav', 'data\\\\speech\\\\DISGUST_5-2-1-004.wav', 'data\\\\speech\\\\DISGUST_5-2-1-005.wav', 'data\\\\speech\\\\DISGUST_5-2-1-006.wav', 'data\\\\speech\\\\DISGUST_5-2-1-007.wav', 'data\\\\speech\\\\DISGUST_5-2-1-008.wav', 'data\\\\speech\\\\DISGUST_5-2-1-009.wav', 'data\\\\speech\\\\DISGUST_5-2-1-010.wav', 'data\\\\speech\\\\DISGUST_5-2-1-011.wav', 'data\\\\speech\\\\DISGUST_5-2-1-012.wav', 'data\\\\speech\\\\DISGUST_5-2-1-013.wav', 'data\\\\speech\\\\DISGUST_5-2-1-014.wav', 'data\\\\speech\\\\DISGUST_5-2-1-015.wav', 'data\\\\speech\\\\DISGUST_5-2-1-016.wav', 'data\\\\speech\\\\DISGUST_5-2-1-017.wav', 'data\\\\speech\\\\DISGUST_5-2-1-018.wav', 'data\\\\speech\\\\DISGUST_5-2-1-019.wav', 'data\\\\speech\\\\DISGUST_5-2-1-020.wav', 'data\\\\speech\\\\DISGUST_5-2-1-021.wav', 'data\\\\speech\\\\DISGUST_5-2-1-022.wav', 'data\\\\speech\\\\DISGUST_5-2-1-023.wav', 'data\\\\speech\\\\DISGUST_5-2-1-024.wav', 'data\\\\speech\\\\DISGUST_5-2-1-025.wav', 'data\\\\speech\\\\DISGUST_5-2-1-026.wav', 'data\\\\speech\\\\DISGUST_5-2-1-027.wav', 'data\\\\speech\\\\DISGUST_5-2-1-028.wav', 'data\\\\speech\\\\DISGUST_5-2-1-029.wav', 'data\\\\speech\\\\DISGUST_5-2-1-030.wav', 'data\\\\speech\\\\DISGUST_5-2-1-031.wav', 'data\\\\speech\\\\DISGUST_5-2-1-032.wav', 'data\\\\speech\\\\DISGUST_5-2-1-033.wav', 'data\\\\speech\\\\DISGUST_5-2-1-034.wav', 'data\\\\speech\\\\DISGUST_5-2-1-035.wav', 'data\\\\speech\\\\DISGUST_5-2-1-036.wav', 'data\\\\speech\\\\DISGUST_5-2-1-037.wav', 'data\\\\speech\\\\DISGUST_5-2-1-038.wav', 'data\\\\speech\\\\DISGUST_5-2-1-039.wav', 'data\\\\speech\\\\DISGUST_5-2-1-040.wav', 'data\\\\speech\\\\DISGUST_5-2-1-041.wav', 'data\\\\speech\\\\DISGUST_5-2-1-042.wav', 'data\\\\speech\\\\DISGUST_5-2-1-043.wav', 'data\\\\speech\\\\DISGUST_5-2-1-044.wav', 'data\\\\speech\\\\DISGUST_5-2-1-045.wav', 'data\\\\speech\\\\DISGUST_5-2-1-046.wav', 'data\\\\speech\\\\DISGUST_5-2-1-047.wav', 'data\\\\speech\\\\DISGUST_5-2-1-048.wav', 'data\\\\speech\\\\DISGUST_5-2-1-049.wav', 'data\\\\speech\\\\DISGUST_5-2-1-050.wav']\n","(100, 98304)\n","HAPPY\n","['data\\\\speech\\\\HAPPY_2-2-0-001.wav', 'data\\\\speech\\\\HAPPY_2-2-0-002.wav', 'data\\\\speech\\\\HAPPY_2-2-0-003.wav', 'data\\\\speech\\\\HAPPY_2-2-0-004.wav', 'data\\\\speech\\\\HAPPY_2-2-0-005.wav', 'data\\\\speech\\\\HAPPY_2-2-0-006.wav', 'data\\\\speech\\\\HAPPY_2-2-0-007.wav', 'data\\\\speech\\\\HAPPY_2-2-0-008.wav', 'data\\\\speech\\\\HAPPY_2-2-0-009.wav', 'data\\\\speech\\\\HAPPY_2-2-0-010.wav', 'data\\\\speech\\\\HAPPY_2-2-0-011.wav', 'data\\\\speech\\\\HAPPY_2-2-0-012.wav', 'data\\\\speech\\\\HAPPY_2-2-0-013.wav', 'data\\\\speech\\\\HAPPY_2-2-0-014.wav', 'data\\\\speech\\\\HAPPY_2-2-0-015.wav', 'data\\\\speech\\\\HAPPY_2-2-0-016.wav', 'data\\\\speech\\\\HAPPY_2-2-0-017.wav', 'data\\\\speech\\\\HAPPY_2-2-0-018.wav', 'data\\\\speech\\\\HAPPY_2-2-0-019.wav', 'data\\\\speech\\\\HAPPY_2-2-0-020.wav', 'data\\\\speech\\\\HAPPY_2-2-0-021.wav', 'data\\\\speech\\\\HAPPY_2-2-0-022.wav', 'data\\\\speech\\\\HAPPY_2-2-0-023.wav', 'data\\\\speech\\\\HAPPY_2-2-0-024.wav', 'data\\\\speech\\\\HAPPY_2-2-0-025.wav', 'data\\\\speech\\\\HAPPY_2-2-0-026.wav', 'data\\\\speech\\\\HAPPY_2-2-0-027.wav', 'data\\\\speech\\\\HAPPY_2-2-0-028.wav', 'data\\\\speech\\\\HAPPY_2-2-0-029.wav', 'data\\\\speech\\\\HAPPY_2-2-0-030.wav', 'data\\\\speech\\\\HAPPY_2-2-0-031.wav', 'data\\\\speech\\\\HAPPY_2-2-0-032.wav', 'data\\\\speech\\\\HAPPY_2-2-0-033.wav', 'data\\\\speech\\\\HAPPY_2-2-0-034.wav', 'data\\\\speech\\\\HAPPY_2-2-0-035.wav', 'data\\\\speech\\\\HAPPY_2-2-0-036.wav', 'data\\\\speech\\\\HAPPY_2-2-0-037.wav', 'data\\\\speech\\\\HAPPY_2-2-0-038.wav', 'data\\\\speech\\\\HAPPY_2-2-0-039.wav', 'data\\\\speech\\\\HAPPY_2-2-0-040.wav', 'data\\\\speech\\\\HAPPY_2-2-0-041.wav', 'data\\\\speech\\\\HAPPY_2-2-0-042.wav', 'data\\\\speech\\\\HAPPY_2-2-0-043.wav', 'data\\\\speech\\\\HAPPY_2-2-0-044.wav', 'data\\\\speech\\\\HAPPY_2-2-0-045.wav', 'data\\\\speech\\\\HAPPY_2-2-0-046.wav', 'data\\\\speech\\\\HAPPY_2-2-0-047.wav', 'data\\\\speech\\\\HAPPY_2-2-0-048.wav', 'data\\\\speech\\\\HAPPY_2-2-0-049.wav', 'data\\\\speech\\\\HAPPY_2-2-0-050.wav', 'data\\\\speech\\\\HAPPY_2-2-1-001.wav', 'data\\\\speech\\\\HAPPY_2-2-1-002.wav', 'data\\\\speech\\\\HAPPY_2-2-1-003.wav', 'data\\\\speech\\\\HAPPY_2-2-1-004.wav', 'data\\\\speech\\\\HAPPY_2-2-1-005.wav', 'data\\\\speech\\\\HAPPY_2-2-1-006.wav', 'data\\\\speech\\\\HAPPY_2-2-1-007.wav', 'data\\\\speech\\\\HAPPY_2-2-1-008.wav', 'data\\\\speech\\\\HAPPY_2-2-1-009.wav', 'data\\\\speech\\\\HAPPY_2-2-1-010.wav', 'data\\\\speech\\\\HAPPY_2-2-1-011.wav', 'data\\\\speech\\\\HAPPY_2-2-1-012.wav', 'data\\\\speech\\\\HAPPY_2-2-1-013.wav', 'data\\\\speech\\\\HAPPY_2-2-1-014.wav', 'data\\\\speech\\\\HAPPY_2-2-1-015.wav', 'data\\\\speech\\\\HAPPY_2-2-1-016.wav', 'data\\\\speech\\\\HAPPY_2-2-1-017.wav', 'data\\\\speech\\\\HAPPY_2-2-1-018.wav', 'data\\\\speech\\\\HAPPY_2-2-1-019.wav', 'data\\\\speech\\\\HAPPY_2-2-1-020.wav', 'data\\\\speech\\\\HAPPY_2-2-1-021.wav', 'data\\\\speech\\\\HAPPY_2-2-1-022.wav', 'data\\\\speech\\\\HAPPY_2-2-1-023.wav', 'data\\\\speech\\\\HAPPY_2-2-1-024.wav', 'data\\\\speech\\\\HAPPY_2-2-1-025.wav', 'data\\\\speech\\\\HAPPY_2-2-1-026.wav', 'data\\\\speech\\\\HAPPY_2-2-1-027.wav', 'data\\\\speech\\\\HAPPY_2-2-1-028.wav', 'data\\\\speech\\\\HAPPY_2-2-1-029.wav', 'data\\\\speech\\\\HAPPY_2-2-1-030.wav', 'data\\\\speech\\\\HAPPY_2-2-1-031.wav', 'data\\\\speech\\\\HAPPY_2-2-1-032.wav', 'data\\\\speech\\\\HAPPY_2-2-1-033.wav', 'data\\\\speech\\\\HAPPY_2-2-1-034.wav', 'data\\\\speech\\\\HAPPY_2-2-1-035.wav', 'data\\\\speech\\\\HAPPY_2-2-1-036.wav', 'data\\\\speech\\\\HAPPY_2-2-1-037.wav', 'data\\\\speech\\\\HAPPY_2-2-1-038.wav', 'data\\\\speech\\\\HAPPY_2-2-1-039.wav', 'data\\\\speech\\\\HAPPY_2-2-1-040.wav', 'data\\\\speech\\\\HAPPY_2-2-1-041.wav', 'data\\\\speech\\\\HAPPY_2-2-1-042.wav', 'data\\\\speech\\\\HAPPY_2-2-1-043.wav', 'data\\\\speech\\\\HAPPY_2-2-1-044.wav', 'data\\\\speech\\\\HAPPY_2-2-1-045.wav', 'data\\\\speech\\\\HAPPY_2-2-1-046.wav', 'data\\\\speech\\\\HAPPY_2-2-1-047.wav', 'data\\\\speech\\\\HAPPY_2-2-1-048.wav', 'data\\\\speech\\\\HAPPY_2-2-1-049.wav', 'data\\\\speech\\\\HAPPY_2-2-1-050.wav']\n","(100, 98304)\n","NEUTRAL\n","['data\\\\speech\\\\NEUTRAL_1-1-0-001.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-002.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-003.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-004.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-005.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-006.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-007.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-008.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-009.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-010.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-011.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-012.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-013.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-014.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-015.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-016.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-017.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-018.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-019.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-020.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-021.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-022.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-023.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-024.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-025.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-026.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-027.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-028.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-029.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-030.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-031.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-032.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-033.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-034.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-035.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-036.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-037.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-038.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-039.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-040.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-041.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-042.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-043.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-044.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-045.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-046.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-047.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-048.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-049.wav', 'data\\\\speech\\\\NEUTRAL_1-1-0-050.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-001.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-002.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-003.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-004.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-005.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-006.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-007.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-008.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-009.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-010.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-011.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-012.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-013.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-014.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-015.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-016.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-017.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-018.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-019.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-020.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-021.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-022.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-023.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-024.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-025.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-026.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-027.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-028.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-029.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-030.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-031.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-032.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-033.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-034.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-035.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-036.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-037.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-038.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-039.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-040.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-041.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-042.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-043.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-044.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-045.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-046.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-047.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-048.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-049.wav', 'data\\\\speech\\\\NEUTRAL_1-1-1-050.wav']\n","(100, 98304)\n","SAD\n","['data\\\\speech\\\\SAD_3-2-0-001.wav', 'data\\\\speech\\\\SAD_3-2-0-002.wav', 'data\\\\speech\\\\SAD_3-2-0-003.wav', 'data\\\\speech\\\\SAD_3-2-0-004.wav', 'data\\\\speech\\\\SAD_3-2-0-005.wav', 'data\\\\speech\\\\SAD_3-2-0-006.wav', 'data\\\\speech\\\\SAD_3-2-0-007.wav', 'data\\\\speech\\\\SAD_3-2-0-008.wav', 'data\\\\speech\\\\SAD_3-2-0-009.wav', 'data\\\\speech\\\\SAD_3-2-0-010.wav', 'data\\\\speech\\\\SAD_3-2-0-011.wav', 'data\\\\speech\\\\SAD_3-2-0-012.wav', 'data\\\\speech\\\\SAD_3-2-0-013.wav', 'data\\\\speech\\\\SAD_3-2-0-014.wav', 'data\\\\speech\\\\SAD_3-2-0-015.wav', 'data\\\\speech\\\\SAD_3-2-0-016.wav', 'data\\\\speech\\\\SAD_3-2-0-017.wav', 'data\\\\speech\\\\SAD_3-2-0-018.wav', 'data\\\\speech\\\\SAD_3-2-0-019.wav', 'data\\\\speech\\\\SAD_3-2-0-020.wav', 'data\\\\speech\\\\SAD_3-2-0-021.wav', 'data\\\\speech\\\\SAD_3-2-0-022.wav', 'data\\\\speech\\\\SAD_3-2-0-023.wav', 'data\\\\speech\\\\SAD_3-2-0-024.wav', 'data\\\\speech\\\\SAD_3-2-0-025.wav', 'data\\\\speech\\\\SAD_3-2-0-026.wav', 'data\\\\speech\\\\SAD_3-2-0-027.wav', 'data\\\\speech\\\\SAD_3-2-0-028.wav', 'data\\\\speech\\\\SAD_3-2-0-029.wav', 'data\\\\speech\\\\SAD_3-2-0-030.wav', 'data\\\\speech\\\\SAD_3-2-0-031.wav', 'data\\\\speech\\\\SAD_3-2-0-032.wav', 'data\\\\speech\\\\SAD_3-2-0-033.wav', 'data\\\\speech\\\\SAD_3-2-0-034.wav', 'data\\\\speech\\\\SAD_3-2-0-035.wav', 'data\\\\speech\\\\SAD_3-2-0-036.wav', 'data\\\\speech\\\\SAD_3-2-0-037.wav', 'data\\\\speech\\\\SAD_3-2-0-038.wav', 'data\\\\speech\\\\SAD_3-2-0-039.wav', 'data\\\\speech\\\\SAD_3-2-0-040.wav', 'data\\\\speech\\\\SAD_3-2-0-041.wav', 'data\\\\speech\\\\SAD_3-2-0-042.wav', 'data\\\\speech\\\\SAD_3-2-0-043.wav', 'data\\\\speech\\\\SAD_3-2-0-044.wav', 'data\\\\speech\\\\SAD_3-2-0-045.wav', 'data\\\\speech\\\\SAD_3-2-0-046.wav', 'data\\\\speech\\\\SAD_3-2-0-047.wav', 'data\\\\speech\\\\SAD_3-2-0-048.wav', 'data\\\\speech\\\\SAD_3-2-0-049.wav', 'data\\\\speech\\\\SAD_3-2-0-050.wav', 'data\\\\speech\\\\SAD_3-2-1-001.wav', 'data\\\\speech\\\\SAD_3-2-1-002.wav', 'data\\\\speech\\\\SAD_3-2-1-003.wav', 'data\\\\speech\\\\SAD_3-2-1-004.wav', 'data\\\\speech\\\\SAD_3-2-1-005.wav', 'data\\\\speech\\\\SAD_3-2-1-006.wav', 'data\\\\speech\\\\SAD_3-2-1-007.wav', 'data\\\\speech\\\\SAD_3-2-1-008.wav', 'data\\\\speech\\\\SAD_3-2-1-009.wav', 'data\\\\speech\\\\SAD_3-2-1-010.wav', 'data\\\\speech\\\\SAD_3-2-1-011.wav', 'data\\\\speech\\\\SAD_3-2-1-012.wav', 'data\\\\speech\\\\SAD_3-2-1-013.wav', 'data\\\\speech\\\\SAD_3-2-1-014.wav', 'data\\\\speech\\\\SAD_3-2-1-015.wav', 'data\\\\speech\\\\SAD_3-2-1-016.wav', 'data\\\\speech\\\\SAD_3-2-1-017.wav', 'data\\\\speech\\\\SAD_3-2-1-018.wav', 'data\\\\speech\\\\SAD_3-2-1-019.wav', 'data\\\\speech\\\\SAD_3-2-1-020.wav', 'data\\\\speech\\\\SAD_3-2-1-021.wav', 'data\\\\speech\\\\SAD_3-2-1-022.wav', 'data\\\\speech\\\\SAD_3-2-1-023.wav', 'data\\\\speech\\\\SAD_3-2-1-024.wav', 'data\\\\speech\\\\SAD_3-2-1-025.wav', 'data\\\\speech\\\\SAD_3-2-1-026.wav', 'data\\\\speech\\\\SAD_3-2-1-027.wav', 'data\\\\speech\\\\SAD_3-2-1-028.wav', 'data\\\\speech\\\\SAD_3-2-1-029.wav', 'data\\\\speech\\\\SAD_3-2-1-030.wav', 'data\\\\speech\\\\SAD_3-2-1-031.wav', 'data\\\\speech\\\\SAD_3-2-1-032.wav', 'data\\\\speech\\\\SAD_3-2-1-033.wav', 'data\\\\speech\\\\SAD_3-2-1-034.wav', 'data\\\\speech\\\\SAD_3-2-1-035.wav', 'data\\\\speech\\\\SAD_3-2-1-036.wav', 'data\\\\speech\\\\SAD_3-2-1-037.wav', 'data\\\\speech\\\\SAD_3-2-1-038.wav', 'data\\\\speech\\\\SAD_3-2-1-039.wav', 'data\\\\speech\\\\SAD_3-2-1-040.wav', 'data\\\\speech\\\\SAD_3-2-1-041.wav', 'data\\\\speech\\\\SAD_3-2-1-042.wav', 'data\\\\speech\\\\SAD_3-2-1-043.wav', 'data\\\\speech\\\\SAD_3-2-1-044.wav', 'data\\\\speech\\\\SAD_3-2-1-045.wav', 'data\\\\speech\\\\SAD_3-2-1-046.wav', 'data\\\\speech\\\\SAD_3-2-1-047.wav', 'data\\\\speech\\\\SAD_3-2-1-048.wav', 'data\\\\speech\\\\SAD_3-2-1-049.wav', 'data\\\\speech\\\\SAD_3-2-1-050.wav']\n","(100, 98304)\n"]}],"source":["#Data Process And save\n","from numpy.core.fromnumeric import shape\n","import librosa\n","import pathlib\n","import numpy as np\n","import pandas as pd\n","import csv\n","# import pywt\n","from sklearn.decomposition import PCA\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","\n","def get_cwt_mel(path, n_fft, hop_length, n_mels):\n","    y, sr = librosa.load(path,sr=16000,duration=8)\n","    file_length = np.size(y)\n","    if file_length < 128000:\n","        y = np.concatenate((y, np.zeros(128000-file_length)), axis=0)\n","    else:\n","        y=y[0:128000]\n","\n","    #mel_spectrogram = librosa.feature.melspectrogram(y, sr, n_fft=2048, hop_length=1001, n_mels=128)\n","    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=501,n_mfcc=128)\n","    stft = np.abs(librosa.stft(y,n_fft=254, hop_length=501))\n","    chroma=librosa.feature.chroma_stft(S=stft,n_chroma=128)\n","    log_mel_spectrogram = np.concatenate((stft,mfcc,chroma), axis=1)\n","    log_mel_spectrogram = log_mel_spectrogram.reshape((-1,))\n","    return log_mel_spectrogram\n","\n","def classify_files(path):\n","    dataset_dict = {\n","        'total': 0,\n","        'file_dict': {\n","            'ANGRY': {'represent': 0, 'count': 0, 'all_data': []},\n","            'DISGUST': {'represent': 1, 'count': 0, 'all_data': []},\n","            'HAPPY': {'represent': 2, 'count': 0, 'all_data': []},\n","            'NEUTRAL': {'represent': 3, 'count': 0, 'all_data': []},\n","            'SAD': {'represent': 4, 'count': 0, 'all_data': []},\n","            # 'FEAR': {'represent': 5, 'count': 0, 'all_data': []},\n","            # 'SURPRISE': {'represent': 6, 'count': 0, 'all_data': []}\n","        }\n","    }    \n","\n","   \n","    wav_path = pathlib.Path(path)\n","    print('wav path: ', wav_path)\n","    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n","    print('emotion files: ', emotion_file_list)\n","\n","    p = len(str(wav_path))\n","\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    print(emotion_label_list)\n","\n","    for emotion_label in emotion_label_list:\n","        print(emotion_label)\n","        \n","        emotion_classify_file_list = [letter for letter in emotion_file_list if letter.find(emotion_label)!=-1]\n","\n","        print(emotion_classify_file_list)\n","        files_count = len(emotion_classify_file_list)\n","\n","        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n","        dataset_dict['total'] = dataset_dict['total'] + files_count\n","        \n","        emotion_data = [get_cwt_mel(path, n_fft=2048, hop_length=512, n_mels=128)\n","\n","        \n","        for path in emotion_classify_file_list] \n","        print(shape(emotion_data))\n","        dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data \n","\n","    return dataset_dict\n","\n","path = './data/speech'\n","dataset_dict = classify_files(path)\n","\n","\n","# "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ANGRY\n","(100, 98304)\n","ANGRY\n","(80, 128, 256, 3)\n","(20, 128, 256, 3)\n","DISGUST\n","(100, 98304)\n","DISGUST\n","(160, 128, 256, 3)\n","(40, 128, 256, 3)\n","HAPPY\n","(100, 98304)\n","HAPPY\n","(240, 128, 256, 3)\n","(60, 128, 256, 3)\n","NEUTRAL\n","(100, 98304)\n","NEUTRAL\n","(320, 128, 256, 3)\n","(80, 128, 256, 3)\n","SAD\n","(100, 98304)\n","SAD\n","(400, 128, 256, 3)\n","(100, 128, 256, 3)\n"]}],"source":["def load_data(path):\n","    train_data_x = []\n","    train_data_y = []\n","    validation_data_x = []\n","    validation_data_y = []\n","    test_data_x = []\n","    test_data_y = []\n","\n","    dataset_dict = classify_files(path)\n","\n","    '''Split data set'''\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        x = dataset_dict['file_dict'][emotion_label]['all_data']\n","        count = dataset_dict['file_dict'][emotion_label]['count']\n","        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n","\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.80, random_state=1)\n","\n","        train_data_x = np.append(train_data_x, x_train)\n","        train_data_y = np.append(train_data_y, y_train)\n","\n","        test_data_x = np.append(test_data_x, x_test)\n","        test_data_y = np.append(test_data_y, y_test)\n","    '''\n","    train_data_x=np.array(train_data_x).reshape(len(train_data_y),-1)\n","    with open(path +'/data/train_data_x.csv',\"w+\") as my_csv:\n","      csvWriter = csv.writer(my_csv,delimiter=',')\n","      csvWriter.writerows(train_data_x)\n","\n","    test_data_x=np.array(test_data_x).reshape(len(test_data_y),-1)\n","    with open(path +'/data/test_data_x.csv',\"w+\") as my_csv:\n","      csvWriter = csv.writer(my_csv,delimiter=',')\n","      csvWriter.writerows(test_data_x)\n","    ''' \n","   \n","\n","    #np.savetxt(path +'/data/train_data_x.csv', np.array(train_data_x))\n","    np.savetxt(path +'/data/train_data_y.csv', train_data_y, delimiter=',')\n","   #np.savetxt(path +'/data/test_data_x.csv', test_data_x, delimiter=',')\n","    np.savetxt(path +'/data/test_data_y.csv', test_data_y, delimiter=',')\n","    \n","    x=128 #128\n","    y=128#768 #384 #256\n","    z=5\n","    train_data_x = np.array(train_data_x).reshape(-1, x, y,3)\n","    #train_data_x = np.array(train_data_x).reshape(-1, x, y, z,1)\n","    train_data_y = np.array(train_data_y)\n","    #test_data_x = np.array(test_data_x).reshape(-1, x, y, z,1)\n","    test_data_x = np.array(test_data_x).reshape(-1, x, y,3)\n","    test_data_y = np.array(test_data_y)\n","\n","    return train_data_x,train_data_y,test_data_x,test_data_y \n","\n","def classify_files_new(path):\n","    dataset_dict = {\n","        'total': 0,\n","        'file_dict': {\n","            'ANGRY': {'represent': 0, 'count': 0, 'all_data': []},\n","            'DISGUST': {'represent': 1, 'count': 0, 'all_data': []},\n","            'HAPPY': {'represent': 2, 'count': 0, 'all_data': []},\n","            'NEUTRAL': {'represent': 3, 'count': 0, 'all_data': []},\n","            'SAD': {'represent': 4, 'count': 0, 'all_data': []},\n","            # 'FEAR': {'represent': 5, 'count': 0, 'all_data': []},\n","            # 'SURPRISE': {'represent': 6, 'count': 0, 'all_data': []}\n","        }\n","    }\n","\n","    wav_path = pathlib.Path(path)\n","    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n","\n","    p = len(str(wav_path))\n","    train_data_x = []\n","    train_data_y = []\n","    test_data_x = []\n","    test_data_y = []\n","    total = []\n","    temp = 0\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        print(emotion_label)\n","\n","        emotion_classify_file_list = [letter for letter in emotion_file_list if letter.find(emotion_label) != -1]\n","\n","        #print(emotion_classify_file_list)\n","        files_count = len(emotion_classify_file_list)\n","\n","        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n","        dataset_dict['total'] = dataset_dict['total'] + files_count\n","\n","        emotion_data = [get_cwt_mel(path, n_fft=2048, hop_length=512, n_mels=128) for path in\n","                        emotion_classify_file_list]\n","\n","        print(shape(emotion_data))\n","        # dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data\n","\n","        x = emotion_data\n","        count = dataset_dict['file_dict'][emotion_label]['count']\n","        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n","        dataset_dict['file_dict'][emotion_label]['represent'] = []\n","        dataset_dict['file_dict'][emotion_label]['count'] = []\n","        emotion_data = []\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.80, random_state=1)\n","\n","        x = []\n","        y = []\n","        x = 128\n","        y = 256\n","        z = 3\n","        print(emotion_label)\n","        train_data_x = np.append(train_data_x, np.array(x_train)).reshape(-1, x, y, z)\n","        print(train_data_x.shape)\n","        test_data_x = np.append(test_data_x, np.array(x_test)).reshape(-1, x, y, z)\n","        print(test_data_x.shape)\n","        \n","        \n","        # print(total.shape)\n","        train_data_y = np.append(train_data_y, y_train)\n","        test_data_y = np.append(test_data_y, y_test)\n","        \n","\n","        print(emotion_label)\n","        x_train=np.array(x_train).reshape(len(y_train),-1)\n","        with open(path +'/data/'+ emotion_label+'_train_data_x.csv',\"w+\") as my_csv:\n","            csvWriter = csv.writer(my_csv,delimiter=',')\n","            csvWriter.writerows(x_train)\n","        my_csv=[]\n","        \n","        x_train=[]\n","        y_train = []\n","        '''\n","        x_test=np.array(x_test).reshape(len(y_test),-1)\n","        with open(path +'/data/'+ emotion_label+'_test_data_x.csv',\"w+\") as my_csv:\n","            csvWriter = csv.writer(my_csv,delimiter=',')\n","            csvWriter.writerows(x_test)\n","        my_csv = []\n","       '''     \n","        x_test=[]\n","        y_test = []\n","        \n","    np.savetxt(path + '/csv/train_data_y1.csv', train_data_y, delimiter=',')\n","    np.savetxt(path + '/csv/test_data_y1.csv', test_data_y, delimiter=',')\n","    #train_data_y = []\n","    #test_data_y = []\n","    return train_data_x, train_data_y, test_data_x, test_data_y\n","\n","train_data_x, train_data_y, test_data_x, test_data_y = classify_files_new(path)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train data x:  (400, 128, 256, 3)\n","train data y:  (400,)\n","test data x:  (100, 128, 256, 3)\n","test data y:  (100,)\n"]}],"source":["print('train data x: ', train_data_x.shape)\n","print('train data y: ', train_data_y.shape)\n","print('test data x: ', test_data_x.shape)\n","print('test data y: ', test_data_y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L366kfiutd3r"},"outputs":[],"source":["import numpy as np\n","def splitCVSData(path,firstIndex,lastIndex):\n","  with open(path) as fd:\n","    reader=csv.reader(fd)\n","    print(reader)\n","    return np.array([row for idx, row in enumerate(reader) if idx in range (firstIndex,lastIndex)])     \n","\n","def dataReshap(path):\n","    x=128 #128\n","    y=256#384 #256\n","    #train_data_x = np.array(np.genfromtxt(path +'/data/train_data_x.csv', delimiter=',')).reshape(-1, 128, 256, 8,1)\n","    #test_data_x = np.array(np.genfromtxt(path +'/data/test_data_x.csv', delimiter=',')).reshape(-1, 128, 256, 8,1)\n","    #temp=np.genfromtxt(path +'/data/train_data_x.csv', delimiter=',')\n","    #print(temp[0])\n","\n","    train_data_x = np.array(np.genfromtxt(path +'/data/train_data_x.csv', delimiter=','))\n","    print('train_data_x.csv')\n","    train_data_y = np.array(np.genfromtxt(path +'/data/train_data_y.csv', delimiter=','))\n","    print('train_data_y.csv')\n","   \n","    test_data_x = np.array(np.genfromtxt(path +'/data/test_data_x.csv', delimiter=','))\n","    print('test_data_x.csv')\n","    test_data_y = np.array(np.genfromtxt(path +'/data/test_data_y.csv', delimiter=','))\n","    print('test_data_y.csv')\n","\n","    #return train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y\n","    return train_data_x.reshape(-1, x, y,3), train_data_y,test_data_x.reshape(-1, x, y, 3), test_data_y\n","\n","def dataReshapNew(path):\n","    x=128 #128\n","    y=513#384 #256\n","    emotion=['ANGRY','DISGUST','FEAR','HAPPY','NEUTRAL','SAD','SURPRISE']\n","    print(emotion[0])\n","    train_data_x =np.array(np.genfromtxt(path +'/data/'+ emotion[0]+'_train_data_x.csv', delimiter=','))\n","    print(train_data_x.shape)\n","    for i in range(1,len(emotion)):\n","      print(emotion[i])\n","      train_data_x =np.concatenate(train_data_x, np.array(np.genfromtxt(path +'/data/'+ emotion[i]+'_train_data_x.csv', delimiter=',')))\n","      print(train_data_x.shape) \n","    \n","    \n","    '''\n","    train_data_y = np.array(np.genfromtxt(path +'/data/train_data_y.csv', delimiter=','))\n","    print('train_data_y.csv')\n","   \n","    test_data_x = np.array(np.genfromtxt(path +'/data/test_data_x.csv', delimiter=','))\n","    print('test_data_x.csv')\n","    test_data_y = np.array(np.genfromtxt(path +'/data/test_data_y.csv', delimiter=','))\n","    print('test_data_y.csv')\n","\n","    #return train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y\n","    return train_data_x.reshape(-1, x, y,3), train_data_y,test_data_x.reshape(-1, x, y, 3), test_data_y\n","    '''\n","\n","path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/RAVDESS/Speech/Actor_01'\n","path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/RAVDESS/All'\n","path='/content/drive/MyDrive/MSc KUET/Thesis/Dataset/RAVDESS/All'\n","\n","path='/content/drive/MyDrive/MSc KUET/Thesis/Dataset//SUBESCO'\n","dataReshapNew(path)"]},{"cell_type":"markdown","metadata":{"id":"d6BLMvfOXnzi"},"source":["##Model SUST"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Kbzr908mUXfV"},"outputs":[],"source":["from numpy.core.fromnumeric import shape\n","import librosa\n","import pathlib\n","import numpy as np\n","# import pywt\n","import csv\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.utils import normalize, to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","import tensorflow as tf\n","from tensorflow import keras,nn\n","from tensorflow.keras import layers\n","\n","def model3d(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model3d')\n","\n","    #LFLB1\n","    model.add(layers.Conv3D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n","    \n","    #LFLB2\n","    model.add(layers.Conv3D(filters=64,kernel_size=3,strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling3D(pool_size=(4,4,1), strides=(4,4,1)))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB3\n","    model.add(layers.Conv3D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling3D(pool_size=(4,4,1), strides=(4,4,1)))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    \n","    #LFLB4\n","    model.add(layers.Conv3D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling3D(pool_size=(4,4,1), strides=(4,4,1)))\n","\n","    #model.add(layers.Reshape((-1, 128)))\n","    model.add(layers.TimeDistributed(layers.Flatten()))\n","    \n","    #LSTM\n","    #model.add(layers.LSTM(256))\n","    model.add(layers.Bidirectional(layers.LSTM(256)))\n","    \n","    #model.add(keras.layers.Dense(128,activation=nn.relu))\n","    #model.add(layers.Dense(128,activation=nn.relu))\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","def model2dv2(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=64,kernel_size=(3,3),strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","    \n","    #LFLB2\n","    model.add(layers.Conv2D(filters=64,kernel_size=(3,3),strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=(4,4), strides=(4,4)))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=128,kernel_size=(3,3),strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=(4,4), strides=(4,4)))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","\n","    \n","    #LFLB4\n","    model.add(layers.Conv2D(filters=128,kernel_size=(3,3),strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=(4,4), strides=(4,4)))\n","\n","    #model.add(layers.Reshape((-1, 128)))\n","    model.add(layers.TimeDistributed(layers.Flatten()))\n","    \n","    #LSTM\n","    #model.add(layers.LSTM(256))\n","    model.add(layers.Bidirectional(layers.LSTM(256)))\n","    \n","    #model.add(keras.layers.Dense(128,activation=nn.relu))\n","    #model.add(layers.Dense(128,activation=nn.relu))\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","def model2d(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","    \n","    #LFLB2\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","\n","    \n","    #LFLB4\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #model.add(layers.Reshape((-1, 128)))\n","    model.add(layers.TimeDistributed(layers.Flatten()))\n","    \n","    #LSTM\n","    #model.add(layers.LSTM(256))\n","    model.add(layers.Bidirectional(layers.LSTM(256)))\n","    \n","    #model.add(keras.layers.Dense(128,activation=nn.relu))\n","    #model.add(layers.Dense(128,activation=nn.relu))\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","def model2CNN2F(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=8, strides=8))\n","\n","    #model.add(layers.Reshape((-1, 128)))\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(128,activation=nn.relu))\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","\n","#physical_device = tf.config.experimental.list_physical_devices(\"GPU\")\n","#tf.config.experimental.set_memory_growth(physical_device[0], True)\n","\n","\n","#def train(train_data_x, train_data_y, validation_data_x, validation_data_y,emotion,emotionNumber):\n","def train(train_data_x, train_data_y,emotion,emotionNumber):\n","    #model = model2dv2(input_shape=(128, 512, 1), num_classes=emotionNumber)\n","    model = model2d(input_shape=(128,256, 3), num_classes=emotionNumber)\n","    #model = model3d(input_shape=(128,128,3,1), num_classes=emotionNumber)\n","    \n","    #model = model2CNN2F(input_shape=(128, 256, 1), num_classes=emotionNumber)\n","    \n","    model.summary()\n","    es = EarlyStopping(monitor='val_categorical_accuracy',mode='max',verbose=1,patience=20)\n","\n","    mc = ModelCheckpoint(path+'/model/'+emotion+'_max_model1.h5',monitor='val_categorical_accuracy',mode='max',verbose=1,save_best_only=True)\n","    history=model.fit(train_data_x, train_data_y,validation_data=(test_data_x, test_data_y),epochs=120,batch_size=10,verbose=2,callbacks=[es,mc])\n","    #vacc=history.history['val_categorical_accuracy'][len(history.history['val_categorical_accuracy'])-1]\n","    #mc = ModelCheckpoint(path+'/model/'+emotion+'_model.h5',mode='max',verbose=0,save_best_only=True)\n","    #history=model.fit(train_data_x, train_data_y,epochs=50,batch_size=20,verbose=2,callbacks=[mc])\n","    acc=history.history['categorical_accuracy'][len(history.history['categorical_accuracy']) - 1]\n","    #model.save(path+'/model/'+emotion+'_model.h5')\n","    return acc\n","\n","def test(test_data_x, test_data_y,emotion):\n","    \n","    new_model = load_model(path+'/model/'+emotion+'_max_model.h5')\n","    history=new_model.evaluate(test_data_x, test_data_y, batch_size=1)\n","    predict=new_model.predict(test_data_x)\n","    return history[1]\n","\n","def maxIndex(data):\n","  max=data[0]\n","  index=0\n","  for i in range(1,len(data)):\n","    if(max<data[i]):\n","      max=data[i]\n","      index=i\n","  return index  \n","\n","def test_emotion(test_data_x, test_data_y,total,emotion):\n","    new_model = load_model(path+'/model/'+emotion+'_max_model.h5')\n","    #test_data_y=to_categorical(test_data_y)\n","    history=new_model.evaluate(test_data_x, test_data_y, batch_size=10)\n","    predict=new_model.predict(test_data_x)\n","    test_data_y=np.argmax(test_data_y, axis=1)\n","\n","    count=[[0]*total]*total\n","    count=np.array(count)\n","    #print(count)\n","    for i in range(0,len(test_data_y)):\n","      predictNew=maxIndex(predict[i])\n","      #print(str(test_data_y[i])+' -- '+ str(predictNew))\n","      #print()\n","      count[test_data_y[i]][predictNew]=count[test_data_y[i]][predictNew]+1\n","\n","    with open(path +'/data/'+emotion+'_confution.csv',\"w+\") as my_csv:\n","      csvWriter = csv.writer(my_csv,delimiter=',')\n","      csvWriter.writerows(count)\n","    print(count)\n"]},{"cell_type":"markdown","metadata":{"id":"ZtcNv73vXrG5"},"source":["##7 Emotion"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"lPBVB0TVXqia"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Web_Projects\\html\\Officials\\Backend\\AI\\Bangla-Speech-Emotion-Recognition\\.conda\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model2d\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"model2d\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m36,928\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │           \u001b[38;5;34m256\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m147,584\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,050,624\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m2,565\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,314,885</span> (5.02 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,314,885\u001b[0m (5.02 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,314,117</span> (5.01 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,314,117\u001b[0m (5.01 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["d:\\Web_Projects\\html\\Officials\\Backend\\AI\\Bangla-Speech-Emotion-Recognition\\.conda\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n","  warnings.warn(\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model2d\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"model2d\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m36,928\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │           \u001b[38;5;34m256\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m147,584\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,050,624\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m2,565\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,314,885</span> (5.02 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,314,885\u001b[0m (5.02 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,314,117</span> (5.01 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,314,117\u001b[0m (5.01 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"]},"metadata":{},"output_type":"display_data"},{"ename":"ValueError","evalue":"The filepath provided must end in `.keras` (Keras model format). Received: filepath=./data/speech/model/5_emotion_max_model1.h5","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5_emotion\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#acc,vacc=train(train_data_x, train_data_y, validation_data_x, validation_data_y,emotion,9)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43memotion\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5_emotion\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m test_emotion(test_data_x, test_data_y,\u001b[38;5;241m5\u001b[39m,emotion)\n","Cell \u001b[1;32mIn[8], line 215\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_data_x, train_data_y, emotion, emotionNumber)\u001b[0m\n\u001b[0;32m    212\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m    213\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m--> 215\u001b[0m mc \u001b[38;5;241m=\u001b[39m \u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/model/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43memotion\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_max_model1.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_categorical_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m history\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(train_data_x, train_data_y,validation_data\u001b[38;5;241m=\u001b[39m(test_data_x, test_data_y),epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,callbacks\u001b[38;5;241m=\u001b[39m[es,mc])\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m#vacc=history.history['val_categorical_accuracy'][len(history.history['val_categorical_accuracy'])-1]\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m#mc = ModelCheckpoint(path+'/model/'+emotion+'_model.h5',mode='max',verbose=0,save_best_only=True)\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m#history=model.fit(train_data_x, train_data_y,epochs=50,batch_size=20,verbose=2,callbacks=[mc])\u001b[39;00m\n","File \u001b[1;32md:\\Web_Projects\\html\\Officials\\Backend\\AI\\Bangla-Speech-Emotion-Recognition\\.conda\\lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[1;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filepath provided must end in `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras model format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         )\n","\u001b[1;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=./data/speech/model/5_emotion_max_model1.h5"]}],"source":["# train_data_y = np.array(np.genfromtxt(path +'/data/train_data_y1.csv', delimiter=','))\n","# #validation_data_y = np.array(np.genfromtxt(path +'/data/validation_data_y.csv', delimiter=','))\n","# test_data_y = np.array(np.genfromtxt(path +'/data/test_data_y1.csv', delimiter=','))\n","\n","train_data_y = to_categorical(train_data_y)\n","#validation_data_y = to_categorical(validation_data_y)\n","test_data_y = to_categorical(test_data_y)\n","emotion='5_emotion'\n","#acc,vacc=train(train_data_x, train_data_y, validation_data_x, validation_data_y,emotion,9)\n","acc=train(train_data_x, train_data_y,emotion,5)\n","\n","emotion='5_emotion'\n","test_emotion(test_data_x, test_data_y,5,emotion)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgnjC9Y-ednN"},"outputs":[],"source":["emotion='7_emotion'\n","test_emotion(train_data_x, train_data_y,7,emotion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9s6BeyigPUyk"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# use raw time-domain speech signal as input to cnn for SER\n","\n","import numpy as np\n","import os\n","import librosa\n","\n","\n","import librosa\n","import pathlib\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","EmoDB_file_path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/SUBESCO'\n","#EmoDB_file_path = '/content/drive/MyDrive/MSc KUET/Thesis/Dataset/SUBESCO' \n","def get_log_mel_spectrogram(path, n_fft, hop_length, n_mels):\n","    \"\"\"\n","    Extract log mel spectrogram\n","        1) The length of the raw audio used is 8s long,\n","        2) and then get the MelSpectrogram,\n","        2) finally perform logarithmic operation to MelSpectrogram.\n","\n","    Return:\n","        log_mel_spectrogram:\n","    \"\"\"\n","    y, sr = librosa.load(path, sr=16000, duration=3)\n","\n","    file_length = np.size(y)\n","    if file_length != 128000:\n","        y = np.concatenate((y, np.zeros(128000-file_length)), axis=0)\n","\n","        scal = np.arange(1, 129)  #513\n","    #mel_spectrogram, frequ = pywt.cwt(y, scal, \"morl\")\n","    #for i in range (0,128):\n","     #mel_spectrogram[i]=preprocessing.normalize([mel_spectrogram[i]])\n","      #max1=max(mel_spectrogram[i])\n","      #min1=min(mel_spectrogram[i])\n","      #mel_spectrogram[i]=(mel_spectrogram[i] - min1) / (max1 - min1)\n","      #print(mel_spectrogram[i])\n","    \n","    #print(mel_spectrogram)\n","    mel_spectrogram = librosa.feature.melspectrogram(y, sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n","    #n_fft=2048, hop_length=512, n_mels=128\n","    #print(shape(mel_spectrogram))\n","    #mel_spectrogram=np.abs(librosa.stft(y, n_fft=254))\n","    #print(shape(mel_spectrogram))\n","    #print(\"-------------------------------------------------------\")\n","    log_mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram)\n","    log_mel_spectrogram = log_mel_spectrogram.reshape((-1,))\n","\n","    return log_mel_spectrogram\n","\n","\n","def classify_files(path):\n","    \"\"\"\n","    Classify emotion files and count them.\n","        Position 6 of emotion file name represent emotion which according to the label as follow:\n","        ( Emotion label letter used german word.)\n","        ----------------------------\n","           letter   |   emotion(En)\n","        ------------+---------------\n","              W     |   anger\n","              L     |   boredom\n","              E     |   disgust\n","              A     |   anxiety/fear\n","              F     |   happiness\n","              T     |   sadness\n","        ------------+---------------\n","\n","    Dataset preprocessing.\n","        Dataset data are divided into\n","\n","    Return:\n","        dataset_dict:\n","            a dict structure with 'total' used to count all file number, and a sub-dict named 'file_dict' which including three keys,\n","    \"\"\"\n","    dataset_dict = {\n","        'total': 0,\n","        'file_dict': {\n","            'ANGRY': {'represent': 0, 'count': 0, 'all_data': []},\n","            'DISGUST': {'represent': 1, 'count': 0, 'all_data': []},\n","            'FEAR': {'represent': 2, 'count': 0, 'all_data': []},\n","            'HAPPY': {'represent': 3, 'count': 0, 'all_data': []},\n","            'NEUTRAL': {'represent': 4, 'count': 0, 'all_data': []},\n","            'SAD': {'represent': 5, 'count': 0, 'all_data': []},\n","            'SURPRISE': {'represent': 6, 'count': 0, 'all_data': []}\n","        }\n","    }\n","\n","    wav_path = pathlib.Path(path)\n","    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n","\n","    p = len(str(wav_path))\n","    #print(emotion_file_list)\n","\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","\n","    for emotion_label in emotion_label_list:\n","        #print(emotion_label)\n","        #print(emotion_file_list[0])\n","        #print(emotion_file_list[0].find(emotion_label))\n","        #actor = ['F_01','F_02','F_03','F_04','F_05','F_06','F_07','F_08','F_09','F_10','M_01','M_02','M_03','M_04','M_05','M_06','M_07','M_08','M_09','M_10']\n","\n","        emotion_classify_file_list = [letter for letter in emotion_file_list if letter.find(emotion_label)!=-1 and (letter.find('F_02')!=-1 or letter.find('F_04')!=-1 or letter.find('F_06')!=-1 or letter.find('F_07')!=-1 or letter.find('M_03')!=-1 or letter.find('M_06')!=-1 or letter.find('M_09')!=-1 or letter.find('M_10')!=-1)]\n","        print(emotion_classify_file_list)\n","        files_count = len(emotion_classify_file_list)\n","\n","        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n","        dataset_dict['total'] = dataset_dict['total'] + files_count\n","        emotion_data = [get_log_mel_spectrogram(path, n_fft=2048, hop_length=512, n_mels=128)\n","                        for path in emotion_classify_file_list]\n","        dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data\n","\n","    return dataset_dict\n","\n","\n","def load_data(path):\n","    \"\"\"\n","\n","    Returns:\n","        train_data_x, train_data_y:\n","            The emotion data and label of train data, which account for 80% in all.\n","        validation_data_x, validation_data_y:\n","            The emotion data and label of validation data, which account for 80% in train data.\n","        test_data_x, test_data_y:\n","            The emotion data and label of test data, which account for 20% in all.\n","    \"\"\"\n","    train_data_x = []\n","    train_data_y = []\n","    validation_data_x = []\n","    validation_data_y = []\n","    test_data_x = []\n","    test_data_y = []\n","\n","    dataset_dict = classify_files(path)\n","\n","    '''Split data set'''\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        x = dataset_dict['file_dict'][emotion_label]['all_data']\n","        count = dataset_dict['file_dict'][emotion_label]['count']\n","        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n","\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8)\n","\n","        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8)\n","\n","        train_data_x = np.append(train_data_x, x_train)\n","        train_data_y = np.append(train_data_y, y_train)\n","\n","        validation_data_x = np.append(validation_data_x, x_val)\n","        validation_data_y = np.append(validation_data_y, y_val)\n","\n","        test_data_x = np.append(test_data_x, x_test)\n","        test_data_y = np.append(test_data_y, y_test)\n","\n","    '''Reshape all data'''\n","    train_data_x = np.array(train_data_x).reshape(-1, 128, 251, 1)\n","    train_data_y = np.array(train_data_y)\n","    validation_data_x = np.array(validation_data_x).reshape(-1, 128, 251, 1)\n","    validation_data_y = np.array(validation_data_y)\n","    test_data_x = np.array(test_data_x).reshape(-1, 128, 251, 1)\n","    test_data_y = np.array(test_data_y)\n","\n","    return train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","\n","def model2d(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB2\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB4\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    model.add(layers.Reshape((-1, 128)))\n","\n","    #LSTM\n","    model.add(layers.LSTM(256))\n","\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.utils import normalize, to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","\n","#physical_device = tf.config.experimental.list_physical_devices(\"GPU\")\n","#tf.config.experimental.set_memory_growth(physical_device[0], True)\n","\n","\n","\n","\n","def train(train_data_x, train_data_y, validation_data_x, validation_data_y):\n","    model = model2d(input_shape=(128, 251, 1), num_classes=7)\n","    model.summary()\n","    es = EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=20)\n","\n","    mc = ModelCheckpoint(EmoDB_file_path+'/model.h5',monitor='val_categorical_accuracy',mode='max',verbose=0,save_best_only=True)\n","\n","    model.fit(train_data_x, train_data_y,validation_data=(validation_data_x, validation_data_y),epochs=200,batch_size=4,verbose=2,callbacks=[es, mc])\n","\n","'''\n","def test(test_data_x, test_data_y ):\n","    new_model = load_model('model.h5')\n","    new_model.evaluate(test_data_x, test_data_y, batch_size=1)'''\n","\n","\n","if __name__ == '__main__':\n","\n","    train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_file_path)\n","\n","    train_data_x = normalize(train_data_x)\n","    validation_data_x = normalize(validation_data_x)\n","    test_data_x = normalize(test_data_x)\n","\n","    train_data_y = to_categorical(train_data_y)\n","    validation_data_y = to_categorical(validation_data_y)\n","    test_data_y = to_categorical(test_data_y)\n","\n","    train(train_data_x, train_data_y, validation_data_x, validation_data_y)\n","\n","    #test(test_data_x, test_data_y)\n","\n","\n","#EmoDB_file_path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/SUBESCO'\n","#train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_fi"]},{"cell_type":"markdown","metadata":{"id":"hr3td_G5PSLX"},"source":["SFFT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KgA1iWn8uKM"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# use raw time-domain speech signal as input to cnn for SER\n","\n","import numpy as np\n","import os\n","import librosa\n","\n","\n","import librosa\n","import pathlib\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","EmoDB_file_path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/SUBESCO'\n","#EmoDB_file_path = '/content/drive/MyDrive/MSc KUET/Thesis/Dataset/SUBESCO' \n","def get_log_mel_spectrogram(path, n_fft, hop_length, n_mels):\n","    \"\"\"\n","    Extract log mel spectrogram\n","        1) The length of the raw audio used is 8s long,\n","        2) and then get the MelSpectrogram,\n","        2) finally perform logarithmic operation to MelSpectrogram.\n","\n","    Return:\n","        log_mel_spectrogram:\n","    \"\"\"\n","    y, sr = librosa.load(path, sr=16000, duration=3)\n","\n","    file_length = np.size(y)\n","    if file_length != 128000:\n","        y = np.concatenate((y, np.zeros(128000-file_length)), axis=0)\n","\n","        scal = np.arange(1, 129)  #513\n","    #mel_spectrogram, frequ = pywt.cwt(y, scal, \"morl\")\n","    #for i in range (0,128):\n","     #mel_spectrogram[i]=preprocessing.normalize([mel_spectrogram[i]])\n","      #max1=max(mel_spectrogram[i])\n","      #min1=min(mel_spectrogram[i])\n","      #mel_spectrogram[i]=(mel_spectrogram[i] - min1) / (max1 - min1)\n","      #print(mel_spectrogram[i])\n","    \n","    #print(mel_spectrogram)\n","    #mel_spectrogram = librosa.feature.melspectrogram(y, sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n","    #n_fft=2048, hop_length=512, n_mels=128\n","    #print(shape(mel_spectrogram))\n","    mel_spectrogram=np.abs(librosa.stft(y, n_fft=254))\n","    #print(shape(mel_spectrogram))\n","    #print(\"-------------------------------------------------------\")\n","    log_mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram)\n","    log_mel_spectrogram = log_mel_spectrogram.reshape((-1,))\n","\n","    return log_mel_spectrogram\n","\n","\n","def classify_files(path):\n","    \"\"\"\n","    Classify emotion files and count them.\n","        Position 6 of emotion file name represent emotion which according to the label as follow:\n","        ( Emotion label letter used german word.)\n","        ----------------------------\n","           letter   |   emotion(En)\n","        ------------+---------------\n","              W     |   anger\n","              L     |   boredom\n","              E     |   disgust\n","              A     |   anxiety/fear\n","              F     |   happiness\n","              T     |   sadness\n","        ------------+---------------\n","\n","    Dataset preprocessing.\n","        Dataset data are divided into\n","\n","    Return:\n","        dataset_dict:\n","            a dict structure with 'total' used to count all file number, and a sub-dict named 'file_dict' which including three keys,\n","    \"\"\"\n","    dataset_dict = {\n","        'total': 0,\n","        'file_dict': {\n","            'ANGRY': {'represent': 0, 'count': 0, 'all_data': []},\n","            'DISGUST': {'represent': 1, 'count': 0, 'all_data': []},\n","            'FEAR': {'represent': 2, 'count': 0, 'all_data': []},\n","            'HAPPY': {'represent': 3, 'count': 0, 'all_data': []},\n","            'NEUTRAL': {'represent': 4, 'count': 0, 'all_data': []},\n","            'SAD': {'represent': 5, 'count': 0, 'all_data': []},\n","            'SURPRISE': {'represent': 6, 'count': 0, 'all_data': []}\n","        }\n","    }\n","\n","    wav_path = pathlib.Path(path)\n","    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n","\n","    p = len(str(wav_path))\n","    #print(emotion_file_list)\n","\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","\n","    for emotion_label in emotion_label_list:\n","        #print(emotion_label)\n","        #print(emotion_file_list[0])\n","        #print(emotion_file_list[0].find(emotion_label))\n","        #actor = ['F_01','F_02','F_03','F_04','F_05','F_06','F_07','F_08','F_09','F_10','M_01','M_02','M_03','M_04','M_05','M_06','M_07','M_08','M_09','M_10']\n","\n","        emotion_classify_file_list = [letter for letter in emotion_file_list if letter.find(emotion_label)!=-1 and (letter.find('F_02')!=-1 or letter.find('F_04')!=-1 or letter.find('F_06')!=-1 or letter.find('F_07')!=-1 or letter.find('M_03')!=-1 or letter.find('M_06')!=-1 or letter.find('M_09')!=-1 or letter.find('M_10')!=-1)]\n","        print(emotion_classify_file_list)\n","        files_count = len(emotion_classify_file_list)\n","\n","        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n","        dataset_dict['total'] = dataset_dict['total'] + files_count\n","        emotion_data = [get_log_mel_spectrogram(path, n_fft=2048, hop_length=512, n_mels=128)\n","                        for path in emotion_classify_file_list]\n","        dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data\n","\n","    return dataset_dict\n","\n","\n","def load_data(path):\n","    \"\"\"\n","\n","    Returns:\n","        train_data_x, train_data_y:\n","            The emotion data and label of train data, which account for 80% in all.\n","        validation_data_x, validation_data_y:\n","            The emotion data and label of validation data, which account for 80% in train data.\n","        test_data_x, test_data_y:\n","            The emotion data and label of test data, which account for 20% in all.\n","    \"\"\"\n","    train_data_x = []\n","    train_data_y = []\n","    validation_data_x = []\n","    validation_data_y = []\n","    test_data_x = []\n","    test_data_y = []\n","\n","    dataset_dict = classify_files(path)\n","\n","    '''Split data set'''\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        x = dataset_dict['file_dict'][emotion_label]['all_data']\n","        count = dataset_dict['file_dict'][emotion_label]['count']\n","        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n","\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.9)\n","\n","        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.9)\n","\n","        train_data_x = np.append(train_data_x, x_train)\n","        train_data_y = np.append(train_data_y, y_train)\n","\n","        validation_data_x = np.append(validation_data_x, x_val)\n","        validation_data_y = np.append(validation_data_y, y_val)\n","\n","        test_data_x = np.append(test_data_x, x_test)\n","        test_data_y = np.append(test_data_y, y_test)\n","\n","    '''Reshape all data'''\n","    train_data_x = np.array(train_data_x).reshape(-1, 128, 2032, 1)\n","    train_data_y = np.array(train_data_y)\n","    validation_data_x = np.array(validation_data_x).reshape(-1, 128, 2032, 1)\n","    validation_data_y = np.array(validation_data_y)\n","    test_data_x = np.array(test_data_x).reshape(-1, 128, 2032, 1)\n","    test_data_y = np.array(test_data_y)\n","\n","    return train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","\n","def model2d(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB2\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB4\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    model.add(layers.Reshape((-1, 128)))\n","\n","    #LSTM\n","    model.add(layers.LSTM(256))\n","\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.utils import normalize, to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","\n","#physical_device = tf.config.experimental.list_physical_devices(\"GPU\")\n","#tf.config.experimental.set_memory_growth(physical_device[0], True)\n","\n","\n","\n","\n","def train(train_data_x, train_data_y, validation_data_x, validation_data_y):\n","    model = model2d(input_shape=(128, 2032, 1), num_classes=7)\n","    model.summary()\n","    es = EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=20)\n","\n","    mc = ModelCheckpoint(EmoDB_file_path+'/model.h5',monitor='val_categorical_accuracy',mode='max',verbose=0,save_best_only=True)\n","\n","    model.fit(train_data_x, train_data_y,validation_data=(validation_data_x, validation_data_y),epochs=200,batch_size=4,verbose=2,callbacks=[es, mc])\n","\n","'''\n","def test(test_data_x, test_data_y ):\n","    new_model = load_model('model.h5')\n","    new_model.evaluate(test_data_x, test_data_y, batch_size=1)'''\n","\n","\n","if __name__ == '__main__':\n","\n","    train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_file_path)\n","\n","    train_data_x = normalize(train_data_x)\n","    validation_data_x = normalize(validation_data_x)\n","    test_data_x = normalize(test_data_x)\n","\n","    train_data_y = to_categorical(train_data_y)\n","    validation_data_y = to_categorical(validation_data_y)\n","    test_data_y = to_categorical(test_data_y)\n","\n","    train(train_data_x, train_data_y, validation_data_x, validation_data_y)\n","\n","    #test(test_data_x, test_data_y)\n","\n","\n","#EmoDB_file_path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/SUBESCO'\n","#train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_file_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFcvgSsaV84U"},"outputs":[],"source":["def test(test_data_x, test_data_y ):\n","    new_model = load_model(EmoDB_file_path+'/model.h5')\n","    new_model.evaluate(test_data_x, test_data_y, batch_size=2)\n","test(test_data_x, test_data_y )"]},{"cell_type":"markdown","metadata":{"id":"6HpUbgsovD-n"},"source":["7 Emotion Spiker Dependent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRH8DaJxbyTG"},"outputs":[],"source":["from numpy.core.fromnumeric import shape\n","import librosa\n","import pathlib\n","import numpy as np\n","import pywt\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","drive.mount('/content/drive')\n","EmoDB_file_path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/SUBESCO'\n","def get_log_mel_spectrogram(path, n_fft, hop_length, n_mels):\n","    \"\"\"\n","    Extract log mel spectrogram\n","        1) The length of the raw audio used is 8s long,\n","        2) and then get the MelSpectrogram,\n","        2) finally perform logarithmic operation to MelSpectrogram.\n","\n","    Return:\n","        log_mel_spectrogram:\n","    \"\"\"\n","    y, sr = librosa.load(path, sr=16000, duration=3)\n","\n","    file_length = np.size(y)\n","    if file_length != 128000:\n","        y = np.concatenate((y, np.zeros(128000-file_length)), axis=0)\n","\n","    scal = np.arange(1, 129)  #513\n","    #mel_spectrogram, frequ = pywt.cwt(y, scal, \"morl\")\n","    #for i in range (0,128):\n","     #mel_spectrogram[i]=preprocessing.normalize([mel_spectrogram[i]])\n","      #max1=max(mel_spectrogram[i])\n","      #min1=min(mel_spectrogram[i])\n","      #mel_spectrogram[i]=(mel_spectrogram[i] - min1) / (max1 - min1)\n","      #print(mel_spectrogram[i])\n","    \n","    #print(mel_spectrogram)\n","    #mel_spectrogram = librosa.feature.melspectrogram(y, sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n","    #n_fft=2048, hop_length=512, n_mels=128\n","    #print(shape(mel_spectrogram))\n","    mel_spectrogram=np.abs(librosa.stft(y, n_fft=254))\n","    #print(shape(mel_spectrogram))\n","    #print(\"-------------------------------------------------------\")\n","    log_mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram)\n","    log_mel_spectrogram = log_mel_spectrogram.reshape((-1,))\n","\n","    return log_mel_spectrogram\n","\n","\n","def classify_files(path,actor):\n","    \"\"\"\n","    Classify emotion files and count them.\n","        Position 6 of emotion file name represent emotion which according to the label as follow:\n","        ( Emotion label letter used german word.)\n","        ----------------------------\n","           letter   |   emotion(En)\n","        ------------+---------------\n","              W     |   anger\n","              L     |   boredom\n","              E     |   disgust\n","              A     |   anxiety/fear\n","              F     |   happiness\n","              T     |   sadness\n","        ------------+---------------\n","\n","    Dataset preprocessing.\n","        Dataset data are divided into\n","\n","    Return:\n","        dataset_dict:\n","            a dict structure with 'total' used to count all file number, and a sub-dict named 'file_dict' which including three keys,\n","    \"\"\"\n","    dataset_dict = {\n","        'total': 0,\n","        'file_dict': {\n","            'ANGRY': {'represent': 0, 'count': 0, 'all_data': []},\n","            'DISGUST': {'represent': 1, 'count': 0, 'all_data': []},\n","            'FEAR': {'represent': 2, 'count': 0, 'all_data': []},\n","            'HAPPY': {'represent': 3, 'count': 0, 'all_data': []},\n","            'NEUTRAL': {'represent': 4, 'count': 0, 'all_data': []},\n","            'SAD': {'represent': 5, 'count': 0, 'all_data': []},\n","            'SURPRISE': {'represent': 6, 'count': 0, 'all_data': []}\n","        }\n","    }\n","\n","    wav_path = pathlib.Path(path)\n","    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n","\n","    p = len(str(wav_path))\n","    #print(emotion_file_list)\n","\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","\n","    for emotion_label in emotion_label_list:\n","        #print(emotion_label)\n","        #print(emotion_file_list[0])\n","        #print(emotion_file_list[0].find(emotion_label))\n","        emotion_classify_file_list = [letter for letter in emotion_file_list if letter.find(emotion_label)!=-1 and letter.find(actor)!=-1]\n","        print(emotion_classify_file_list)\n","        files_count = len(emotion_classify_file_list)\n","\n","        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n","        dataset_dict['total'] = dataset_dict['total'] + files_count\n","        emotion_data = [get_log_mel_spectrogram(path, n_fft=2048, hop_length=512, n_mels=128)\n","                        for path in emotion_classify_file_list]\n","        dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data\n","\n","    return dataset_dict\n","\n","\n","def load_data(path,actor):\n","    \"\"\"\n","\n","    Returns:\n","        train_data_x, train_data_y:\n","            The emotion data and label of train data, which account for 80% in all.\n","        validation_data_x, validation_data_y:\n","            The emotion data and label of validation data, which account for 80% in train data.\n","        test_data_x, test_data_y:\n","            The emotion data and label of test data, which account for 20% in all.\n","    \"\"\"\n","    train_data_x = []\n","    train_data_y = []\n","    validation_data_x = []\n","    validation_data_y = []\n","    test_data_x = []\n","    test_data_y = []\n","\n","    dataset_dict = classify_files(path,actor)\n","\n","    '''Split data set'''\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        x = dataset_dict['file_dict'][emotion_label]['all_data']\n","        count = dataset_dict['file_dict'][emotion_label]['count']\n","        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n","\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8)\n","\n","        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8)\n","\n","        train_data_x = np.append(train_data_x, x_train)\n","        train_data_y = np.append(train_data_y, y_train)\n","\n","        validation_data_x = np.append(validation_data_x, x_val)\n","        validation_data_y = np.append(validation_data_y, y_val)\n","\n","        test_data_x = np.append(test_data_x, x_test)\n","        test_data_y = np.append(test_data_y, y_test)\n","\n","    '''Reshape all data'''\n","    train_data_x = np.array(train_data_x).reshape(-1, 128, 2032, 1)\n","    train_data_y = np.array(train_data_y)\n","    validation_data_x = np.array(validation_data_x).reshape(-1, 128, 2032, 1)\n","    validation_data_y = np.array(validation_data_y)\n","    test_data_x = np.array(test_data_x).reshape(-1, 128, 2032, 1)\n","    test_data_y = np.array(test_data_y)\n","\n","    return train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","\n","def model2d(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB2\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB4\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    model.add(layers.Reshape((-1, 128)))\n","\n","    #LSTM\n","    model.add(layers.LSTM(256))\n","\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.utils import normalize, to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","\n","#physical_device = tf.config.experimental.list_physical_devices(\"GPU\")\n","#tf.config.experimental.set_memory_growth(physical_device[0], True)\n","\n","\n","\n","\n","def train(train_data_x, train_data_y, validation_data_x, validation_data_y):\n","    model = model2d(input_shape=(128, 2032, 1), num_classes=7)\n","    model.summary()\n","    es = EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=20)\n","\n","    mc = ModelCheckpoint(EmoDB_file_path+'/model.h5',monitor='val_categorical_accuracy',mode='max',verbose=0,save_best_only=True)\n","\n","    history=model.fit(train_data_x, train_data_y,validation_data=(validation_data_x, validation_data_y),epochs=200,batch_size=4,verbose=2,callbacks=[es, mc])\n","    acc=history.history['val_categorical_accuracy'][len(history.history['val_categorical_accuracy'])-1]\n","    vacc=history.history['categorical_accuracy'][len(history.history['categorical_accuracy']) - 1]\n","    return acc,vacc\n","\n","def test(test_data_x, test_data_y ):\n","    new_model = load_model(EmoDB_file_path+'/model.h5')\n","    history=new_model.evaluate(test_data_x, test_data_y, batch_size=1)\n","    return history[1]\n","\n","\n","if __name__ == '__main__':\n","    result=[]\n","    actor = ['F_01','F_02','F_03','F_04','F_05','F_06','F_07','F_08','F_09','F_10','M_01','M_02','M_03','M_04','M_05','M_06','M_07','M_08','M_09','M_10']\n","\n","    \n","    for i in range (0,20):\n","        train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_file_path,actor[i])\n","        train_data_x = normalize(train_data_x)\n","        validation_data_x = normalize(validation_data_x)\n","        test_data_x = normalize(test_data_x)\n","\n","        train_data_y = to_categorical(train_data_y)\n","        validation_data_y = to_categorical(validation_data_y)\n","        test_data_y = to_categorical(test_data_y)\n","\n","        acc,vacc=train(train_data_x, train_data_y, validation_data_x, validation_data_y)\n","        tacc=test(test_data_x, test_data_y)\n","        print([acc,vacc,tacc])\n","        result.append([acc,vacc,tacc])\n","    print(result)\n","\n","#EmoDB_file_path = 'D:\\Education\\MSc KUET\\Thesis\\Dataset\\SUBESCO'\n","#actor='F_01'\n","#train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_file_path,actor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-dq_ROH_K6_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRR9jkgHvGfu"},"outputs":[],"source":["count=1\n","for i in result:\n","  print('Actor ' + str(count)+' \\t'+str(i[1])+'\\t'+str(i[0])+'\\t'+str(i[2]))\n","  count=count+1"]},{"cell_type":"markdown","metadata":{"id":"kp7dnhdJwbSL"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
