{"cells":[{"cell_type":"markdown","metadata":{"id":"096fBrSoj3z8"},"source":["SUST Bangla"]},{"cell_type":"markdown","metadata":{"id":"Z05HYcen8siN"},"source":["# 7 Emotion Spiker Independent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pge_rpkxTEBX"},"outputs":[],"source":["#Data Process And save\n","from numpy.core.fromnumeric import shape\n","import librosa\n","import pathlib\n","import numpy as np\n","import pandas as pd\n","import csv\n","import pywt\n","from sklearn.decomposition import PCA\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive0\n","\n","def get_cwt_mel(path, n_fft, hop_length, n_mels):\n","    y, sr = librosa.load(path,sr=16000,duration=8)\n","    file_length = np.size(y)\n","    if file_length < 128000:\n","        y = np.concatenate((y, np.zeros(128000-file_length)), axis=0)\n","    else:\n","        y=y[0:128000]\n","\n","    #mel_spectrogram = librosa.feature.melspectrogram(y, sr, n_fft=2048, hop_length=1001, n_mels=128)\n","    mfcc = librosa.feature.mfcc(y, sr, hop_length=501,n_mfcc=128)\n","    stft = np.abs(librosa.stft(y,n_fft=254, hop_length=501))\n","    chroma=librosa.feature.chroma_stft(S=stft,n_chroma=128)\n","    log_mel_spectrogram = np.concatenate((stft,mfcc,chroma), axis=1)\n","    log_mel_spectrogram = log_mel_spectrogram.reshape((-1,))\n","    return log_mel_spectrogram\n","\n","\n","def classify_files(path):\n","    dataset_dict = {\n","        'total': 0,\n","        'file_dict': {\n","            'ANGRY': {'represent': 0, 'count': 0, 'all_data': []},\n","            'DISGUST': {'represent': 1, 'count': 0, 'all_data': []},\n","            'FEAR': {'represent': 2, 'count': 0, 'all_data': []},\n","            'HAPPY': {'represent': 3, 'count': 0, 'all_data': []},\n","            'NEUTRAL': {'represent': 4, 'count': 0, 'all_data': []},\n","            'SAD': {'represent': 5, 'count': 0, 'all_data': []},\n","            'SURPRISE': {'represent': 6, 'count': 0, 'all_data': []}\n","        }\n","    }    \n","\n","   \n","    wav_path = pathlib.Path(path)\n","    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n","\n","    p = len(str(wav_path))\n","\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        print(emotion_label)\n","        \n","        emotion_classify_file_list = [letter for letter in emotion_file_list if letter.find(emotion_label)!=-1]\n","\n","        print(emotion_classify_file_list)\n","        files_count = len(emotion_classify_file_list)\n","\n","        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n","        dataset_dict['total'] = dataset_dict['total'] + files_count\n","        \n","        emotion_data = [get_cwt_mel(path, n_fft=2048, hop_length=512, n_mels=128)\n","\n","        \n","        for path in emotion_classify_file_list] \n","        print(shape(emotion_data))\n","        dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data \n","\n","    return dataset_dict\n","\n","def load_data(path):\n","\n","    train_data_x = []\n","    train_data_y = []\n","    validation_data_x = []\n","    validation_data_y = []\n","    test_data_x = []\n","    test_data_y = []\n","\n","    dataset_dict = classify_files(path)\n","\n","    '''Split data set'''\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        x = dataset_dict['file_dict'][emotion_label]['all_data']\n","        count = dataset_dict['file_dict'][emotion_label]['count']\n","        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n","\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.80, random_state=1)\n","\n","        train_data_x = np.append(train_data_x, x_train)\n","        train_data_y = np.append(train_data_y, y_train)\n","\n","        test_data_x = np.append(test_data_x, x_test)\n","        test_data_y = np.append(test_data_y, y_test)\n","    '''\n","    train_data_x=np.array(train_data_x).reshape(len(train_data_y),-1)\n","    with open(path +'/data/train_data_x.csv',\"w+\") as my_csv:\n","      csvWriter = csv.writer(my_csv,delimiter=',')\n","      csvWriter.writerows(train_data_x)\n","\n","    test_data_x=np.array(test_data_x).reshape(len(test_data_y),-1)\n","    with open(path +'/data/test_data_x.csv',\"w+\") as my_csv:\n","      csvWriter = csv.writer(my_csv,delimiter=',')\n","      csvWriter.writerows(test_data_x)\n","    ''' \n","   \n","\n","    #np.savetxt(path +'/data/train_data_x.csv', np.array(train_data_x))\n","    np.savetxt(path +'/data/train_data_y.csv', train_data_y, delimiter=',')\n","   #np.savetxt(path +'/data/test_data_x.csv', test_data_x, delimiter=',')\n","    np.savetxt(path +'/data/test_data_y.csv', test_data_y, delimiter=',')\n","    \n","    x=128 #128\n","    y=128#768 #384 #256\n","    z=5\n","    train_data_x = np.array(train_data_x).reshape(-1, x, y,3)\n","    #train_data_x = np.array(train_data_x).reshape(-1, x, y, z,1)\n","    train_data_y = np.array(train_data_y)\n","    #test_data_x = np.array(test_data_x).reshape(-1, x, y, z,1)\n","    test_data_x = np.array(test_data_x).reshape(-1, x, y,3)\n","    test_data_y = np.array(test_data_y)\n","\n","    return train_data_x,train_data_y,test_data_x,test_data_y \n","\n","def classify_files_new(path):\n","    dataset_dict = {\n","        'total': 0,\n","        'file_dict': {\n","            'ANGRY': {'represent': 0, 'count': 0, 'all_data': []},\n","            'DISGUST': {'represent': 1, 'count': 0, 'all_data': []},\n","            'FEAR': {'represent': 2, 'count': 0, 'all_data': []},\n","            'HAPPY': {'represent': 3, 'count': 0, 'all_data': []},\n","            'NEUTRAL': {'represent': 4, 'count': 0, 'all_data': []},\n","            'SAD': {'represent': 5, 'count': 0, 'all_data': []},\n","            'SURPRISE': {'represent': 6, 'count': 0, 'all_data': []}\n","        }\n","    }\n","\n","    wav_path = pathlib.Path(path)\n","    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n","\n","    p = len(str(wav_path))\n","    train_data_x = []\n","    train_data_y = []\n","    test_data_x = []\n","    test_data_y = []\n","    total = []\n","    temp = 0\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        print(emotion_label)\n","\n","        emotion_classify_file_list = [letter for letter in emotion_file_list if letter.find(emotion_label) != -1]\n","\n","        #print(emotion_classify_file_list)\n","        files_count = len(emotion_classify_file_list)\n","\n","        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n","        dataset_dict['total'] = dataset_dict['total'] + files_count\n","\n","        emotion_data = [get_cwt_mel(path, n_fft=2048, hop_length=512, n_mels=128) for path in\n","                        emotion_classify_file_list]\n","\n","        print(shape(emotion_data))\n","        # dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data\n","\n","        x = emotion_data\n","        count = dataset_dict['file_dict'][emotion_label]['count']\n","        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n","        dataset_dict['file_dict'][emotion_label]['represent'] = []\n","        dataset_dict['file_dict'][emotion_label]['count'] = []\n","        emotion_data = []\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.80, random_state=1)\n","\n","        x = []\n","        y = []\n","        x = 128\n","        y = 256\n","        z = 3\n","        print(emotion_label)\n","        train_data_x = np.append(train_data_x, np.array(x_train)).reshape(-1, x, y, z)\n","        print(train_data_x.shape)\n","        test_data_x = np.append(test_data_x, np.array(x_test)).reshape(-1, x, y, z)\n","        print(test_data_x.shape)\n","        \n","        \n","        # print(total.shape)\n","        train_data_y = np.append(train_data_y, y_train)\n","        test_data_y = np.append(test_data_y, y_test)\n","        \n","        '''\n","        print(emotion_label)\n","        x_train=np.array(x_train).reshape(len(y_train),-1)\n","        with open(path +'/data/'+ emotion_label+'_train_data_x.csv',\"w+\") as my_csv:\n","            csvWriter = csv.writer(my_csv,delimiter=',')\n","            csvWriter.writerows(x_train)\n","        my_csv=[]\n","        '''\n","        x_train=[]\n","        y_train = []\n","        '''\n","        x_test=np.array(x_test).reshape(len(y_test),-1)\n","        with open(path +'/data/'+ emotion_label+'_test_data_x.csv',\"w+\") as my_csv:\n","            csvWriter = csv.writer(my_csv,delimiter=',')\n","            csvWriter.writerows(x_test)\n","        my_csv = []\n","       '''     \n","        x_test=[]\n","        y_test = []\n","        \n","    np.savetxt(path + '/data/train_data_y1.csv', train_data_y, delimiter=',')\n","    np.savetxt(path + '/data/test_data_y1.csv', test_data_y, delimiter=',')\n","    #train_data_y = []\n","    #test_data_y = []\n","    return train_data_x, train_data_y, test_data_x, test_data_y\n","\n","\n","\n","\n","path = 'D:/Education/MSc KUET/Thesis/Dataset/SUBESCO'\n","path='/content/drive/MyDrive/MSc KUET/Thesis/Dataset//SUBESCO'\n","# train_data_x,train_data_y,test_data_x,test_data_y =load_data(path)\n","# train_data_x,train_data_y,test_data_x,test_data_y =load_data_Paralle(path)\n","train_data_x, train_data_y, test_data_x, test_data_y = classify_files_new(path)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L366kfiutd3r"},"outputs":[],"source":["import numpy as np\n","def splitCVSData(path,firstIndex,lastIndex):\n","  with open(path) as fd:\n","    reader=csv.reader(fd)\n","    print(reader)\n","    return np.array([row for idx, row in enumerate(reader) if idx in range (firstIndex,lastIndex)])     \n","  \n","\n","def dataReshap(path):\n","    x=128 #128\n","    y=256#384 #256\n","    #train_data_x = np.array(np.genfromtxt(path +'/data/train_data_x.csv', delimiter=',')).reshape(-1, 128, 256, 8,1)\n","    #test_data_x = np.array(np.genfromtxt(path +'/data/test_data_x.csv', delimiter=',')).reshape(-1, 128, 256, 8,1)\n","    #temp=np.genfromtxt(path +'/data/train_data_x.csv', delimiter=',')\n","    #print(temp[0])\n","\n","    train_data_x = np.array(np.genfromtxt(path +'/data/train_data_x.csv', delimiter=','))\n","    print('train_data_x.csv')\n","    train_data_y = np.array(np.genfromtxt(path +'/data/train_data_y.csv', delimiter=','))\n","    print('train_data_y.csv')\n","   \n","    test_data_x = np.array(np.genfromtxt(path +'/data/test_data_x.csv', delimiter=','))\n","    print('test_data_x.csv')\n","    test_data_y = np.array(np.genfromtxt(path +'/data/test_data_y.csv', delimiter=','))\n","    print('test_data_y.csv')\n","\n","    #return train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y\n","    return train_data_x.reshape(-1, x, y,3), train_data_y,test_data_x.reshape(-1, x, y, 3), test_data_y\n","\n","def dataReshapNew(path):\n","    x=128 #128\n","    y=513#384 #256\n","    emotion=['ANGRY','DISGUST','FEAR','HAPPY','NEUTRAL','SAD','SURPRISE']\n","    print(emotion[0])\n","    train_data_x =np.array(np.genfromtxt(path +'/data/'+ emotion[0]+'_train_data_x.csv', delimiter=','))\n","    print(train_data_x.shape)\n","    for i in range(1,len(emotion)):\n","      print(emotion[i])\n","      train_data_x =np.concatenate(train_data_x, np.array(np.genfromtxt(path +'/data/'+ emotion[i]+'_train_data_x.csv', delimiter=',')))\n","      print(train_data_x.shape)\n","    \n","    \n","    '''\n","    train_data_y = np.array(np.genfromtxt(path +'/data/train_data_y.csv', delimiter=','))\n","    print('train_data_y.csv')\n","   \n","    test_data_x = np.array(np.genfromtxt(path +'/data/test_data_x.csv', delimiter=','))\n","    print('test_data_x.csv')\n","    test_data_y = np.array(np.genfromtxt(path +'/data/test_data_y.csv', delimiter=','))\n","    print('test_data_y.csv')\n","\n","    #return train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y\n","    return train_data_x.reshape(-1, x, y,3), train_data_y,test_data_x.reshape(-1, x, y, 3), test_data_y\n","    '''\n","\n","path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/RAVDESS/Speech/Actor_01'\n","path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/RAVDESS/All'\n","path='/content/drive/MyDrive/MSc KUET/Thesis/Dataset/RAVDESS/All'\n","\n","path='/content/drive/MyDrive/MSc KUET/Thesis/Dataset//SUBESCO'\n","dataReshapNew(path)"]},{"cell_type":"markdown","metadata":{"id":"d6BLMvfOXnzi"},"source":["##Model SUST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kbzr908mUXfV"},"outputs":[],"source":["from numpy.core.fromnumeric import shape\n","import librosa\n","import pathlib\n","import numpy as np\n","import pywt\n","import csv\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","import tensorflow as tf\n","from tensorflow.keras.utils import normalize, to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","import tensorflow as tf\n","from tensorflow import keras,nn\n","from tensorflow.keras import layers\n","\n","def model3d(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model3d')\n","\n","    #LFLB1\n","    model.add(layers.Conv3D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n","    \n","    #LFLB2\n","    model.add(layers.Conv3D(filters=64,kernel_size=3,strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling3D(pool_size=(4,4,1), strides=(4,4,1)))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB3\n","    model.add(layers.Conv3D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling3D(pool_size=(4,4,1), strides=(4,4,1)))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    \n","    #LFLB4\n","    model.add(layers.Conv3D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling3D(pool_size=(4,4,1), strides=(4,4,1)))\n","\n","    #model.add(layers.Reshape((-1, 128)))\n","    model.add(layers.TimeDistributed(layers.Flatten()))\n","    \n","    #LSTM\n","    #model.add(layers.LSTM(256))\n","    model.add(layers.Bidirectional(layers.LSTM(256)))\n","    \n","    #model.add(keras.layers.Dense(128,activation=nn.relu))\n","    #model.add(layers.Dense(128,activation=nn.relu))\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","\n","\n","def model2dv2(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=64,kernel_size=(3,3),strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","    \n","    #LFLB2\n","    model.add(layers.Conv2D(filters=64,kernel_size=(3,3),strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=(4,4), strides=(4,4)))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=128,kernel_size=(3,3),strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=(4,4), strides=(4,4)))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","\n","    \n","    #LFLB4\n","    model.add(layers.Conv2D(filters=128,kernel_size=(3,3),strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=(4,4), strides=(4,4)))\n","\n","    #model.add(layers.Reshape((-1, 128)))\n","    model.add(layers.TimeDistributed(layers.Flatten()))\n","    \n","    #LSTM\n","    #model.add(layers.LSTM(256))\n","    model.add(layers.Bidirectional(layers.LSTM(256)))\n","    \n","    #model.add(keras.layers.Dense(128,activation=nn.relu))\n","    #model.add(layers.Dense(128,activation=nn.relu))\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","\n","def model2d(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","    \n","    #LFLB2\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","\n","    \n","    #LFLB4\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('relu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #model.add(layers.Reshape((-1, 128)))\n","    model.add(layers.TimeDistributed(layers.Flatten()))\n","    \n","    #LSTM\n","    #model.add(layers.LSTM(256))\n","    model.add(layers.Bidirectional(layers.LSTM(256)))\n","    \n","    #model.add(keras.layers.Dense(128,activation=nn.relu))\n","    #model.add(layers.Dense(128,activation=nn.relu))\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","def model2CNN2F(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=8, strides=8))\n","\n","    #model.add(layers.Reshape((-1, 128)))\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(128,activation=nn.relu))\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","\n","#physical_device = tf.config.experimental.list_physical_devices(\"GPU\")\n","#tf.config.experimental.set_memory_growth(physical_device[0], True)\n","\n","\n","\n","\n","#def train(train_data_x, train_data_y, validation_data_x, validation_data_y,emotion,emotionNumber):\n","def train(train_data_x, train_data_y,emotion,emotionNumber):\n","    #model = model2dv2(input_shape=(128, 512, 1), num_classes=emotionNumber)\n","    model = model2d(input_shape=(128,256, 3), num_classes=emotionNumber)\n","    #model = model3d(input_shape=(128,128,3,1), num_classes=emotionNumber)\n","    \n","    #model = model2CNN2F(input_shape=(128, 256, 1), num_classes=emotionNumber)\n","    \n","    model.summary()\n","    es = EarlyStopping(monitor='val_categorical_accuracy',mode='max',verbose=1,patience=20)\n","\n","    mc = ModelCheckpoint(path+'/model/'+emotion+'_max_model1.h5',monitor='val_categorical_accuracy',mode='max',verbose=1,save_best_only=True)\n","    history=model.fit(train_data_x, train_data_y,validation_data=(test_data_x, test_data_y),epochs=120,batch_size=10,verbose=2,callbacks=[es,mc])\n","    #vacc=history.history['val_categorical_accuracy'][len(history.history['val_categorical_accuracy'])-1]\n","    #mc = ModelCheckpoint(path+'/model/'+emotion+'_model.h5',mode='max',verbose=0,save_best_only=True)\n","    #history=model.fit(train_data_x, train_data_y,epochs=50,batch_size=20,verbose=2,callbacks=[mc])\n","    acc=history.history['categorical_accuracy'][len(history.history['categorical_accuracy']) - 1]\n","    #model.save(path+'/model/'+emotion+'_model.h5')\n","    return acc\n","\n","\n","\n","\n","def test(test_data_x, test_data_y,emotion):\n","    \n","    new_model = load_model(path+'/model/'+emotion+'_max_model.h5')\n","    history=new_model.evaluate(test_data_x, test_data_y, batch_size=1)\n","    predict=new_model.predict(test_data_x)\n","    return history[1]\n","\n","def maxIndex(data):\n","  max=data[0]\n","  index=0\n","  for i in range(1,len(data)):\n","    if(max<data[i]):\n","      max=data[i]\n","      index=i\n","  return index  \n","\n","def test_emotion(test_data_x, test_data_y,total,emotion):\n","    \n","    new_model = load_model(path+'/model/'+emotion+'_max_model.h5')\n","    #test_data_y=to_categorical(test_data_y)\n","    history=new_model.evaluate(test_data_x, test_data_y, batch_size=10)\n","    predict=new_model.predict(test_data_x)\n","    test_data_y=np.argmax(test_data_y, axis=1)\n","\n","    count=[[0]*total]*total\n","    count=np.array(count)\n","    #print(count)\n","    for i in range(0,len(test_data_y)):\n","      predictNew=maxIndex(predict[i])\n","      #print(str(test_data_y[i])+' -- '+ str(predictNew))\n","      #print()\n","      count[test_data_y[i]][predictNew]=count[test_data_y[i]][predictNew]+1\n","\n","    with open(path +'/data/'+emotion+'_confution.csv',\"w+\") as my_csv:\n","      csvWriter = csv.writer(my_csv,delimiter=',')\n","      csvWriter.writerows(count)\n","    print(count)\n"]},{"cell_type":"markdown","metadata":{"id":"ZtcNv73vXrG5"},"source":["##7 Emotion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPBVB0TVXqia"},"outputs":[],"source":["train_data_y = np.array(np.genfromtxt(path +'/data/train_data_y1.csv', delimiter=','))\n","#validation_data_y = np.array(np.genfromtxt(path +'/data/validation_data_y.csv', delimiter=','))\n","test_data_y = np.array(np.genfromtxt(path +'/data/test_data_y1.csv', delimiter=','))\n","\n","train_data_y = to_categorical(train_data_y)\n","#validation_data_y = to_categorical(validation_data_y)\n","test_data_y = to_categorical(test_data_y)\n","emotion='7_emotion'\n","#acc,vacc=train(train_data_x, train_data_y, validation_data_x, validation_data_y,emotion,9)\n","acc=train(train_data_x, train_data_y,emotion,7)\n","\n","emotion='7_emotion'\n","test_emotion(test_data_x, test_data_y,7,emotion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgnjC9Y-ednN"},"outputs":[],"source":["emotion='7_emotion'\n","test_emotion(train_data_x, train_data_y,7,emotion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9s6BeyigPUyk"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# use raw time-domain speech signal as input to cnn for SER\n","\n","import numpy as np\n","import os\n","import librosa\n","\n","\n","import librosa\n","import pathlib\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","EmoDB_file_path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/SUBESCO'\n","#EmoDB_file_path = '/content/drive/MyDrive/MSc KUET/Thesis/Dataset/SUBESCO' \n","def get_log_mel_spectrogram(path, n_fft, hop_length, n_mels):\n","    \"\"\"\n","    Extract log mel spectrogram\n","        1) The length of the raw audio used is 8s long,\n","        2) and then get the MelSpectrogram,\n","        2) finally perform logarithmic operation to MelSpectrogram.\n","\n","    Return:\n","        log_mel_spectrogram:\n","    \"\"\"\n","    y, sr = librosa.load(path, sr=16000, duration=3)\n","\n","    file_length = np.size(y)\n","    if file_length != 128000:\n","        y = np.concatenate((y, np.zeros(128000-file_length)), axis=0)\n","\n","        scal = np.arange(1, 129)  #513\n","    #mel_spectrogram, frequ = pywt.cwt(y, scal, \"morl\")\n","    #for i in range (0,128):\n","     #mel_spectrogram[i]=preprocessing.normalize([mel_spectrogram[i]])\n","      #max1=max(mel_spectrogram[i])\n","      #min1=min(mel_spectrogram[i])\n","      #mel_spectrogram[i]=(mel_spectrogram[i] - min1) / (max1 - min1)\n","      #print(mel_spectrogram[i])\n","    \n","    #print(mel_spectrogram)\n","    mel_spectrogram = librosa.feature.melspectrogram(y, sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n","    #n_fft=2048, hop_length=512, n_mels=128\n","    #print(shape(mel_spectrogram))\n","    #mel_spectrogram=np.abs(librosa.stft(y, n_fft=254))\n","    #print(shape(mel_spectrogram))\n","    #print(\"-------------------------------------------------------\")\n","    log_mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram)\n","    log_mel_spectrogram = log_mel_spectrogram.reshape((-1,))\n","\n","    return log_mel_spectrogram\n","\n","\n","def classify_files(path):\n","    \"\"\"\n","    Classify emotion files and count them.\n","        Position 6 of emotion file name represent emotion which according to the label as follow:\n","        ( Emotion label letter used german word.)\n","        ----------------------------\n","           letter   |   emotion(En)\n","        ------------+---------------\n","              W     |   anger\n","              L     |   boredom\n","              E     |   disgust\n","              A     |   anxiety/fear\n","              F     |   happiness\n","              T     |   sadness\n","        ------------+---------------\n","\n","    Dataset preprocessing.\n","        Dataset data are divided into\n","\n","    Return:\n","        dataset_dict:\n","            a dict structure with 'total' used to count all file number, and a sub-dict named 'file_dict' which including three keys,\n","    \"\"\"\n","    dataset_dict = {\n","        'total': 0,\n","        'file_dict': {\n","            'ANGRY': {'represent': 0, 'count': 0, 'all_data': []},\n","            'DISGUST': {'represent': 1, 'count': 0, 'all_data': []},\n","            'FEAR': {'represent': 2, 'count': 0, 'all_data': []},\n","            'HAPPY': {'represent': 3, 'count': 0, 'all_data': []},\n","            'NEUTRAL': {'represent': 4, 'count': 0, 'all_data': []},\n","            'SAD': {'represent': 5, 'count': 0, 'all_data': []},\n","            'SURPRISE': {'represent': 6, 'count': 0, 'all_data': []}\n","        }\n","    }\n","\n","    wav_path = pathlib.Path(path)\n","    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n","\n","    p = len(str(wav_path))\n","    #print(emotion_file_list)\n","\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","\n","    for emotion_label in emotion_label_list:\n","        #print(emotion_label)\n","        #print(emotion_file_list[0])\n","        #print(emotion_file_list[0].find(emotion_label))\n","        #actor = ['F_01','F_02','F_03','F_04','F_05','F_06','F_07','F_08','F_09','F_10','M_01','M_02','M_03','M_04','M_05','M_06','M_07','M_08','M_09','M_10']\n","\n","        emotion_classify_file_list = [letter for letter in emotion_file_list if letter.find(emotion_label)!=-1 and (letter.find('F_02')!=-1 or letter.find('F_04')!=-1 or letter.find('F_06')!=-1 or letter.find('F_07')!=-1 or letter.find('M_03')!=-1 or letter.find('M_06')!=-1 or letter.find('M_09')!=-1 or letter.find('M_10')!=-1)]\n","        print(emotion_classify_file_list)\n","        files_count = len(emotion_classify_file_list)\n","\n","        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n","        dataset_dict['total'] = dataset_dict['total'] + files_count\n","        emotion_data = [get_log_mel_spectrogram(path, n_fft=2048, hop_length=512, n_mels=128)\n","                        for path in emotion_classify_file_list]\n","        dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data\n","\n","    return dataset_dict\n","\n","\n","def load_data(path):\n","    \"\"\"\n","\n","    Returns:\n","        train_data_x, train_data_y:\n","            The emotion data and label of train data, which account for 80% in all.\n","        validation_data_x, validation_data_y:\n","            The emotion data and label of validation data, which account for 80% in train data.\n","        test_data_x, test_data_y:\n","            The emotion data and label of test data, which account for 20% in all.\n","    \"\"\"\n","    train_data_x = []\n","    train_data_y = []\n","    validation_data_x = []\n","    validation_data_y = []\n","    test_data_x = []\n","    test_data_y = []\n","\n","    dataset_dict = classify_files(path)\n","\n","    '''Split data set'''\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        x = dataset_dict['file_dict'][emotion_label]['all_data']\n","        count = dataset_dict['file_dict'][emotion_label]['count']\n","        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n","\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8)\n","\n","        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8)\n","\n","        train_data_x = np.append(train_data_x, x_train)\n","        train_data_y = np.append(train_data_y, y_train)\n","\n","        validation_data_x = np.append(validation_data_x, x_val)\n","        validation_data_y = np.append(validation_data_y, y_val)\n","\n","        test_data_x = np.append(test_data_x, x_test)\n","        test_data_y = np.append(test_data_y, y_test)\n","\n","    '''Reshape all data'''\n","    train_data_x = np.array(train_data_x).reshape(-1, 128, 251, 1)\n","    train_data_y = np.array(train_data_y)\n","    validation_data_x = np.array(validation_data_x).reshape(-1, 128, 251, 1)\n","    validation_data_y = np.array(validation_data_y)\n","    test_data_x = np.array(test_data_x).reshape(-1, 128, 251, 1)\n","    test_data_y = np.array(test_data_y)\n","\n","    return train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","\n","def model2d(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB2\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB4\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    model.add(layers.Reshape((-1, 128)))\n","\n","    #LSTM\n","    model.add(layers.LSTM(256))\n","\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.utils import normalize, to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","\n","#physical_device = tf.config.experimental.list_physical_devices(\"GPU\")\n","#tf.config.experimental.set_memory_growth(physical_device[0], True)\n","\n","\n","\n","\n","def train(train_data_x, train_data_y, validation_data_x, validation_data_y):\n","    model = model2d(input_shape=(128, 251, 1), num_classes=7)\n","    model.summary()\n","    es = EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=20)\n","\n","    mc = ModelCheckpoint(EmoDB_file_path+'/model.h5',monitor='val_categorical_accuracy',mode='max',verbose=0,save_best_only=True)\n","\n","    model.fit(train_data_x, train_data_y,validation_data=(validation_data_x, validation_data_y),epochs=200,batch_size=4,verbose=2,callbacks=[es, mc])\n","\n","'''\n","def test(test_data_x, test_data_y ):\n","    new_model = load_model('model.h5')\n","    new_model.evaluate(test_data_x, test_data_y, batch_size=1)'''\n","\n","\n","if __name__ == '__main__':\n","\n","    train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_file_path)\n","\n","    train_data_x = normalize(train_data_x)\n","    validation_data_x = normalize(validation_data_x)\n","    test_data_x = normalize(test_data_x)\n","\n","    train_data_y = to_categorical(train_data_y)\n","    validation_data_y = to_categorical(validation_data_y)\n","    test_data_y = to_categorical(test_data_y)\n","\n","    train(train_data_x, train_data_y, validation_data_x, validation_data_y)\n","\n","    #test(test_data_x, test_data_y)\n","\n","\n","#EmoDB_file_path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/SUBESCO'\n","#train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_fi"]},{"cell_type":"markdown","metadata":{"id":"hr3td_G5PSLX"},"source":["SFFT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KgA1iWn8uKM"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# use raw time-domain speech signal as input to cnn for SER\n","\n","import numpy as np\n","import os\n","import librosa\n","\n","\n","import librosa\n","import pathlib\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","EmoDB_file_path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/SUBESCO'\n","#EmoDB_file_path = '/content/drive/MyDrive/MSc KUET/Thesis/Dataset/SUBESCO' \n","def get_log_mel_spectrogram(path, n_fft, hop_length, n_mels):\n","    \"\"\"\n","    Extract log mel spectrogram\n","        1) The length of the raw audio used is 8s long,\n","        2) and then get the MelSpectrogram,\n","        2) finally perform logarithmic operation to MelSpectrogram.\n","\n","    Return:\n","        log_mel_spectrogram:\n","    \"\"\"\n","    y, sr = librosa.load(path, sr=16000, duration=3)\n","\n","    file_length = np.size(y)\n","    if file_length != 128000:\n","        y = np.concatenate((y, np.zeros(128000-file_length)), axis=0)\n","\n","        scal = np.arange(1, 129)  #513\n","    #mel_spectrogram, frequ = pywt.cwt(y, scal, \"morl\")\n","    #for i in range (0,128):\n","     #mel_spectrogram[i]=preprocessing.normalize([mel_spectrogram[i]])\n","      #max1=max(mel_spectrogram[i])\n","      #min1=min(mel_spectrogram[i])\n","      #mel_spectrogram[i]=(mel_spectrogram[i] - min1) / (max1 - min1)\n","      #print(mel_spectrogram[i])\n","    \n","    #print(mel_spectrogram)\n","    #mel_spectrogram = librosa.feature.melspectrogram(y, sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n","    #n_fft=2048, hop_length=512, n_mels=128\n","    #print(shape(mel_spectrogram))\n","    mel_spectrogram=np.abs(librosa.stft(y, n_fft=254))\n","    #print(shape(mel_spectrogram))\n","    #print(\"-------------------------------------------------------\")\n","    log_mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram)\n","    log_mel_spectrogram = log_mel_spectrogram.reshape((-1,))\n","\n","    return log_mel_spectrogram\n","\n","\n","def classify_files(path):\n","    \"\"\"\n","    Classify emotion files and count them.\n","        Position 6 of emotion file name represent emotion which according to the label as follow:\n","        ( Emotion label letter used german word.)\n","        ----------------------------\n","           letter   |   emotion(En)\n","        ------------+---------------\n","              W     |   anger\n","              L     |   boredom\n","              E     |   disgust\n","              A     |   anxiety/fear\n","              F     |   happiness\n","              T     |   sadness\n","        ------------+---------------\n","\n","    Dataset preprocessing.\n","        Dataset data are divided into\n","\n","    Return:\n","        dataset_dict:\n","            a dict structure with 'total' used to count all file number, and a sub-dict named 'file_dict' which including three keys,\n","    \"\"\"\n","    dataset_dict = {\n","        'total': 0,\n","        'file_dict': {\n","            'ANGRY': {'represent': 0, 'count': 0, 'all_data': []},\n","            'DISGUST': {'represent': 1, 'count': 0, 'all_data': []},\n","            'FEAR': {'represent': 2, 'count': 0, 'all_data': []},\n","            'HAPPY': {'represent': 3, 'count': 0, 'all_data': []},\n","            'NEUTRAL': {'represent': 4, 'count': 0, 'all_data': []},\n","            'SAD': {'represent': 5, 'count': 0, 'all_data': []},\n","            'SURPRISE': {'represent': 6, 'count': 0, 'all_data': []}\n","        }\n","    }\n","\n","    wav_path = pathlib.Path(path)\n","    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n","\n","    p = len(str(wav_path))\n","    #print(emotion_file_list)\n","\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","\n","    for emotion_label in emotion_label_list:\n","        #print(emotion_label)\n","        #print(emotion_file_list[0])\n","        #print(emotion_file_list[0].find(emotion_label))\n","        #actor = ['F_01','F_02','F_03','F_04','F_05','F_06','F_07','F_08','F_09','F_10','M_01','M_02','M_03','M_04','M_05','M_06','M_07','M_08','M_09','M_10']\n","\n","        emotion_classify_file_list = [letter for letter in emotion_file_list if letter.find(emotion_label)!=-1 and (letter.find('F_02')!=-1 or letter.find('F_04')!=-1 or letter.find('F_06')!=-1 or letter.find('F_07')!=-1 or letter.find('M_03')!=-1 or letter.find('M_06')!=-1 or letter.find('M_09')!=-1 or letter.find('M_10')!=-1)]\n","        print(emotion_classify_file_list)\n","        files_count = len(emotion_classify_file_list)\n","\n","        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n","        dataset_dict['total'] = dataset_dict['total'] + files_count\n","        emotion_data = [get_log_mel_spectrogram(path, n_fft=2048, hop_length=512, n_mels=128)\n","                        for path in emotion_classify_file_list]\n","        dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data\n","\n","    return dataset_dict\n","\n","\n","def load_data(path):\n","    \"\"\"\n","\n","    Returns:\n","        train_data_x, train_data_y:\n","            The emotion data and label of train data, which account for 80% in all.\n","        validation_data_x, validation_data_y:\n","            The emotion data and label of validation data, which account for 80% in train data.\n","        test_data_x, test_data_y:\n","            The emotion data and label of test data, which account for 20% in all.\n","    \"\"\"\n","    train_data_x = []\n","    train_data_y = []\n","    validation_data_x = []\n","    validation_data_y = []\n","    test_data_x = []\n","    test_data_y = []\n","\n","    dataset_dict = classify_files(path)\n","\n","    '''Split data set'''\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        x = dataset_dict['file_dict'][emotion_label]['all_data']\n","        count = dataset_dict['file_dict'][emotion_label]['count']\n","        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n","\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.9)\n","\n","        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.9)\n","\n","        train_data_x = np.append(train_data_x, x_train)\n","        train_data_y = np.append(train_data_y, y_train)\n","\n","        validation_data_x = np.append(validation_data_x, x_val)\n","        validation_data_y = np.append(validation_data_y, y_val)\n","\n","        test_data_x = np.append(test_data_x, x_test)\n","        test_data_y = np.append(test_data_y, y_test)\n","\n","    '''Reshape all data'''\n","    train_data_x = np.array(train_data_x).reshape(-1, 128, 2032, 1)\n","    train_data_y = np.array(train_data_y)\n","    validation_data_x = np.array(validation_data_x).reshape(-1, 128, 2032, 1)\n","    validation_data_y = np.array(validation_data_y)\n","    test_data_x = np.array(test_data_x).reshape(-1, 128, 2032, 1)\n","    test_data_y = np.array(test_data_y)\n","\n","    return train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","\n","def model2d(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB2\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB4\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    model.add(layers.Reshape((-1, 128)))\n","\n","    #LSTM\n","    model.add(layers.LSTM(256))\n","\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.utils import normalize, to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","\n","#physical_device = tf.config.experimental.list_physical_devices(\"GPU\")\n","#tf.config.experimental.set_memory_growth(physical_device[0], True)\n","\n","\n","\n","\n","def train(train_data_x, train_data_y, validation_data_x, validation_data_y):\n","    model = model2d(input_shape=(128, 2032, 1), num_classes=7)\n","    model.summary()\n","    es = EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=20)\n","\n","    mc = ModelCheckpoint(EmoDB_file_path+'/model.h5',monitor='val_categorical_accuracy',mode='max',verbose=0,save_best_only=True)\n","\n","    model.fit(train_data_x, train_data_y,validation_data=(validation_data_x, validation_data_y),epochs=200,batch_size=4,verbose=2,callbacks=[es, mc])\n","\n","'''\n","def test(test_data_x, test_data_y ):\n","    new_model = load_model('model.h5')\n","    new_model.evaluate(test_data_x, test_data_y, batch_size=1)'''\n","\n","\n","if __name__ == '__main__':\n","\n","    train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_file_path)\n","\n","    train_data_x = normalize(train_data_x)\n","    validation_data_x = normalize(validation_data_x)\n","    test_data_x = normalize(test_data_x)\n","\n","    train_data_y = to_categorical(train_data_y)\n","    validation_data_y = to_categorical(validation_data_y)\n","    test_data_y = to_categorical(test_data_y)\n","\n","    train(train_data_x, train_data_y, validation_data_x, validation_data_y)\n","\n","    #test(test_data_x, test_data_y)\n","\n","\n","#EmoDB_file_path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/SUBESCO'\n","#train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_file_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFcvgSsaV84U"},"outputs":[],"source":["def test(test_data_x, test_data_y ):\n","    new_model = load_model(EmoDB_file_path+'/model.h5')\n","    new_model.evaluate(test_data_x, test_data_y, batch_size=2)\n","test(test_data_x, test_data_y )"]},{"cell_type":"markdown","metadata":{"id":"6HpUbgsovD-n"},"source":["7 Emotion Spiker Dependent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRH8DaJxbyTG"},"outputs":[],"source":["from numpy.core.fromnumeric import shape\n","import librosa\n","import pathlib\n","import numpy as np\n","import pywt\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","drive.mount('/content/drive')\n","EmoDB_file_path = '/content/drive/Othercomputers/My Laptop 07-10-2021/MSc KUET/Thesis/Dataset/SUBESCO'\n","def get_log_mel_spectrogram(path, n_fft, hop_length, n_mels):\n","    \"\"\"\n","    Extract log mel spectrogram\n","        1) The length of the raw audio used is 8s long,\n","        2) and then get the MelSpectrogram,\n","        2) finally perform logarithmic operation to MelSpectrogram.\n","\n","    Return:\n","        log_mel_spectrogram:\n","    \"\"\"\n","    y, sr = librosa.load(path, sr=16000, duration=3)\n","\n","    file_length = np.size(y)\n","    if file_length != 128000:\n","        y = np.concatenate((y, np.zeros(128000-file_length)), axis=0)\n","\n","    scal = np.arange(1, 129)  #513\n","    #mel_spectrogram, frequ = pywt.cwt(y, scal, \"morl\")\n","    #for i in range (0,128):\n","     #mel_spectrogram[i]=preprocessing.normalize([mel_spectrogram[i]])\n","      #max1=max(mel_spectrogram[i])\n","      #min1=min(mel_spectrogram[i])\n","      #mel_spectrogram[i]=(mel_spectrogram[i] - min1) / (max1 - min1)\n","      #print(mel_spectrogram[i])\n","    \n","    #print(mel_spectrogram)\n","    #mel_spectrogram = librosa.feature.melspectrogram(y, sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n","    #n_fft=2048, hop_length=512, n_mels=128\n","    #print(shape(mel_spectrogram))\n","    mel_spectrogram=np.abs(librosa.stft(y, n_fft=254))\n","    #print(shape(mel_spectrogram))\n","    #print(\"-------------------------------------------------------\")\n","    log_mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram)\n","    log_mel_spectrogram = log_mel_spectrogram.reshape((-1,))\n","\n","    return log_mel_spectrogram\n","\n","\n","def classify_files(path,actor):\n","    \"\"\"\n","    Classify emotion files and count them.\n","        Position 6 of emotion file name represent emotion which according to the label as follow:\n","        ( Emotion label letter used german word.)\n","        ----------------------------\n","           letter   |   emotion(En)\n","        ------------+---------------\n","              W     |   anger\n","              L     |   boredom\n","              E     |   disgust\n","              A     |   anxiety/fear\n","              F     |   happiness\n","              T     |   sadness\n","        ------------+---------------\n","\n","    Dataset preprocessing.\n","        Dataset data are divided into\n","\n","    Return:\n","        dataset_dict:\n","            a dict structure with 'total' used to count all file number, and a sub-dict named 'file_dict' which including three keys,\n","    \"\"\"\n","    dataset_dict = {\n","        'total': 0,\n","        'file_dict': {\n","            'ANGRY': {'represent': 0, 'count': 0, 'all_data': []},\n","            'DISGUST': {'represent': 1, 'count': 0, 'all_data': []},\n","            'FEAR': {'represent': 2, 'count': 0, 'all_data': []},\n","            'HAPPY': {'represent': 3, 'count': 0, 'all_data': []},\n","            'NEUTRAL': {'represent': 4, 'count': 0, 'all_data': []},\n","            'SAD': {'represent': 5, 'count': 0, 'all_data': []},\n","            'SURPRISE': {'represent': 6, 'count': 0, 'all_data': []}\n","        }\n","    }\n","\n","    wav_path = pathlib.Path(path)\n","    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n","\n","    p = len(str(wav_path))\n","    #print(emotion_file_list)\n","\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","\n","    for emotion_label in emotion_label_list:\n","        #print(emotion_label)\n","        #print(emotion_file_list[0])\n","        #print(emotion_file_list[0].find(emotion_label))\n","        emotion_classify_file_list = [letter for letter in emotion_file_list if letter.find(emotion_label)!=-1 and letter.find(actor)!=-1]\n","        print(emotion_classify_file_list)\n","        files_count = len(emotion_classify_file_list)\n","\n","        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n","        dataset_dict['total'] = dataset_dict['total'] + files_count\n","        emotion_data = [get_log_mel_spectrogram(path, n_fft=2048, hop_length=512, n_mels=128)\n","                        for path in emotion_classify_file_list]\n","        dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data\n","\n","    return dataset_dict\n","\n","\n","def load_data(path,actor):\n","    \"\"\"\n","\n","    Returns:\n","        train_data_x, train_data_y:\n","            The emotion data and label of train data, which account for 80% in all.\n","        validation_data_x, validation_data_y:\n","            The emotion data and label of validation data, which account for 80% in train data.\n","        test_data_x, test_data_y:\n","            The emotion data and label of test data, which account for 20% in all.\n","    \"\"\"\n","    train_data_x = []\n","    train_data_y = []\n","    validation_data_x = []\n","    validation_data_y = []\n","    test_data_x = []\n","    test_data_y = []\n","\n","    dataset_dict = classify_files(path,actor)\n","\n","    '''Split data set'''\n","    emotion_label_list = dataset_dict['file_dict'].keys()\n","    for emotion_label in emotion_label_list:\n","        x = dataset_dict['file_dict'][emotion_label]['all_data']\n","        count = dataset_dict['file_dict'][emotion_label]['count']\n","        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n","\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8)\n","\n","        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8)\n","\n","        train_data_x = np.append(train_data_x, x_train)\n","        train_data_y = np.append(train_data_y, y_train)\n","\n","        validation_data_x = np.append(validation_data_x, x_val)\n","        validation_data_y = np.append(validation_data_y, y_val)\n","\n","        test_data_x = np.append(test_data_x, x_test)\n","        test_data_y = np.append(test_data_y, y_test)\n","\n","    '''Reshape all data'''\n","    train_data_x = np.array(train_data_x).reshape(-1, 128, 2032, 1)\n","    train_data_y = np.array(train_data_y)\n","    validation_data_x = np.array(validation_data_x).reshape(-1, 128, 2032, 1)\n","    validation_data_y = np.array(validation_data_y)\n","    test_data_x = np.array(test_data_x).reshape(-1, 128, 2032, 1)\n","    test_data_y = np.array(test_data_y)\n","\n","    return train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","\n","def model2d(input_shape, num_classes):\n","\n","    model = keras.Sequential(name='model2d')\n","\n","    #LFLB1\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n","\n","    #LFLB2\n","    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB3\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    #LFLB4\n","    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.Activation('elu'))\n","    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n","\n","    model.add(layers.Reshape((-1, 128)))\n","\n","    #LSTM\n","    model.add(layers.LSTM(256))\n","\n","    model.add(layers.Dense(units=num_classes, activation='softmax'))\n","\n","    model.summary()\n","\n","    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n","\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n","\n","    return model\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.utils import normalize, to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","\n","#physical_device = tf.config.experimental.list_physical_devices(\"GPU\")\n","#tf.config.experimental.set_memory_growth(physical_device[0], True)\n","\n","\n","\n","\n","def train(train_data_x, train_data_y, validation_data_x, validation_data_y):\n","    model = model2d(input_shape=(128, 2032, 1), num_classes=7)\n","    model.summary()\n","    es = EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=20)\n","\n","    mc = ModelCheckpoint(EmoDB_file_path+'/model.h5',monitor='val_categorical_accuracy',mode='max',verbose=0,save_best_only=True)\n","\n","    history=model.fit(train_data_x, train_data_y,validation_data=(validation_data_x, validation_data_y),epochs=200,batch_size=4,verbose=2,callbacks=[es, mc])\n","    acc=history.history['val_categorical_accuracy'][len(history.history['val_categorical_accuracy'])-1]\n","    vacc=history.history['categorical_accuracy'][len(history.history['categorical_accuracy']) - 1]\n","    return acc,vacc\n","\n","def test(test_data_x, test_data_y ):\n","    new_model = load_model(EmoDB_file_path+'/model.h5')\n","    history=new_model.evaluate(test_data_x, test_data_y, batch_size=1)\n","    return history[1]\n","\n","\n","if __name__ == '__main__':\n","    result=[]\n","    actor = ['F_01','F_02','F_03','F_04','F_05','F_06','F_07','F_08','F_09','F_10','M_01','M_02','M_03','M_04','M_05','M_06','M_07','M_08','M_09','M_10']\n","\n","    \n","    for i in range (0,20):\n","        train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_file_path,actor[i])\n","        train_data_x = normalize(train_data_x)\n","        validation_data_x = normalize(validation_data_x)\n","        test_data_x = normalize(test_data_x)\n","\n","        train_data_y = to_categorical(train_data_y)\n","        validation_data_y = to_categorical(validation_data_y)\n","        test_data_y = to_categorical(test_data_y)\n","\n","        acc,vacc=train(train_data_x, train_data_y, validation_data_x, validation_data_y)\n","        tacc=test(test_data_x, test_data_y)\n","        print([acc,vacc,tacc])\n","        result.append([acc,vacc,tacc])\n","    print(result)\n","\n","#EmoDB_file_path = 'D:\\Education\\MSc KUET\\Thesis\\Dataset\\SUBESCO'\n","#actor='F_01'\n","#train_data_x, train_data_y, validation_data_x, validation_data_y, test_data_x, test_data_y = load_data(EmoDB_file_path,actor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-dq_ROH_K6_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRR9jkgHvGfu"},"outputs":[],"source":["count=1\n","for i in result:\n","  print('Actor ' + str(count)+' \\t'+str(i[1])+'\\t'+str(i[0])+'\\t'+str(i[2]))\n","  count=count+1"]},{"cell_type":"markdown","metadata":{"id":"kp7dnhdJwbSL"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
